{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Richardson-Lucy Deconvolution/Imaging Using COSI-Balloon Flight Data: Crab </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "We will need to import the cosipy-classic functions from COSIpy_dc1.py, response_dc1, and COSIpy_tools_dc1, as well as some other standard Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from COSIpy_dc1 import *\n",
    "import response_dc1\n",
    "from COSIpy_tools_dc1 import *\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from tqdm.autonotebook import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define input data and response:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data_products' # directory containing data & response files\n",
    "filename = 'Crab_COSIBalloonData.tra.gz' # combined simulation\n",
    "response_filename = data_dir + '/Continuum_imaging_response.npz' # detector response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading in data and define analysis object</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = COSIpy(data_dir,filename)\n",
    "analysis.read_COSI_DataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bin the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the bin sizes\n",
    "Delta_T = 1800 #30 min\n",
    "energy_bin_edges=np.array([150,  220,  325,  480,  520,  765, 1120, 1650, 2350, 3450, 5000])\n",
    "pixel_size = 6.\n",
    "\n",
    "analysis.dataset.time_binning_tags(time_bin_size=Delta_T) # time binning\n",
    "analysis.dataset.init_binning(energy_bin_edges=energy_bin_edges,pixel_size=pixel_size) # energy and pixel binning\n",
    "analysis.dataset.get_binned_data() # bin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total exposure time of the Crab for this selected data set\n",
    "print('Total Crab exposure in seconds:', analysis.dataset.times.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot a raw-spectrum and light curve<\\h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataset.plot_raw_spectrum()\n",
    "plt.xscale('log')\n",
    "\n",
    "analysis.dataset.plot_lightcurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Select which energy bin to use for the following imaging </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebin 2 is recommended because it has the highest effective area and flux\n",
    "# ebin:             0    |      1    |    2    |    3    |    4     |     5    |     6     |     7     |     8    |     9    |\n",
    "# energies(keV): 150-220 | 220 - 325 | 325-480 | 480-520 | 520-765 | 765-1120 | 1120-1650 | 1650-2350 | 2350-3450| 3450-5000 |\n",
    "\n",
    "#ebin, DeltaE = 0, 220 - 150\n",
    "#ebin, DeltaE = 1, 325 - 220\n",
    "ebin, DeltaE = 2, 480 - 325\n",
    "#ebin, DeltaE = 4, 765 - 520\n",
    "#ebin, DeltaE = 5, 1120 - 765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define the pointing object with the cosipy pointing class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of poitings / pointing object (balloon location + Earth rotation)\n",
    "pointing = Pointing(dataset=analysis.dataset,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualize the path of the Crab through the field-of-view</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crab Galactic coordinates (long, lat)\n",
    "l1,b1 = 184.55746, -5.78436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the position of the Crab during flight, relative to the zenith of the detector\n",
    "plt.figure(figsize=(15,8))\n",
    "l_exp = pointing.zpoins[:,0]\n",
    "b_exp = pointing.zpoins[:,1]\n",
    "l_exp[l_exp > 0] -= 360\n",
    "plt.plot(l_exp,b_exp,'o')\n",
    "plt.plot(l1-360,b1,'*b',markersize=20, label='crab')\n",
    "plt.xlabel('Longitude [deg]')\n",
    "plt.ylabel('Latitude [deg]')\n",
    "plt.title('COSI Instrument Pointings')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot elevation angle of Crab relative to instrument zenith\n",
    "analysis.plot_elevation([l1],[b1],['Crab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Define the BG model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Due to the fact that flight data background is highly variable, we use a background tracer to better track the changes for improved background modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define background tracer\n",
    "tracer = np.sum(analysis.dataset.binned_data,axis=(1,2,3))\n",
    "tracer = tracer/np.mean(tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the background model we can select from models derived from different simulations of the 2016 flight, or a model derived from balloon data. The 'default 6deg' model is derived from the 2016 balloon data. This means we're using the flight data to define a background response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background object defined with background tracer (making the zeros zero)\n",
    "background = BG(dataset=analysis.dataset,mode='default 6deg',tracer=tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading in the continuum detector response for the full 2016 flight:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp = response_dc1.SkyResponse(filename=response_filename, pixel_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print response dimensions\n",
    "print('response dimensions (lat, lon, phi, psi/chi, energy bins):', rsp.rsp.response_grid_normed_efinal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows are some hard-coded sections where we define the sky and background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Defining the sky in pixels to make images:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our sky-grid on a regular 6x6 pixel grid\n",
    "binsize = 6.\n",
    "# Set l and b pixel edges\n",
    "l_arrg = np.linspace(-180,180,int(360/binsize)+1)\n",
    "b_arrg = np.linspace(-90,90,int(180/binsize)+1)\n",
    "# Set number of pixels in l and b\n",
    "n_l = int(360/binsize)\n",
    "n_b = int(180/binsize)\n",
    "# Creating a grid\n",
    "L_ARRg, B_ARRg = np.meshgrid(l_arrg,b_arrg)\n",
    "# Choosing the center points as representative\n",
    "l_arr = l_arrg[0:-1]+binsize/2\n",
    "b_arr = b_arrg[0:-1]+binsize/2\n",
    "L_ARR, B_ARR = np.meshgrid(l_arr,b_arr)\n",
    "\n",
    "# Define solid angle for each pixel for normalisations later\n",
    "deg2rad = np.pi/180.\n",
    "domega = (binsize*deg2rad)*(np.sin(np.deg2rad(B_ARR+binsize/2)) - np.sin(np.deg2rad(B_ARR-binsize/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convert sky grid to zenith/azimuth pairs for all pointings:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the zeniths and azimuths on the defined grid for all times\n",
    "zensgrid,azisgrid = zenaziGrid(pointing.ypoins[:,0],pointing.ypoins[:,1],\n",
    "                               pointing.xpoins[:,0],pointing.xpoins[:,1],\n",
    "                               pointing.zpoins[:,0],pointing.zpoins[:,1],\n",
    "                               L_ARR.ravel(),B_ARR.ravel())\n",
    "# Reshape for the following routines \n",
    "zensgrid = zensgrid.reshape(n_b,n_l,len(pointing.xpoins))\n",
    "azisgrid = azisgrid.reshape(n_b,n_l,len(pointing.xpoins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting the observation indices where we actually have measured photons (important for later):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the int value of pixel indexes that have non-zero values\n",
    "nonzero_idx = background.calc_this[ebin]\n",
    "nonzero_idx_num = nonzero_idx.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Below is a function created to obtain the response of an image for any arbitrary time binning:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_response_from_pixelhit_general(Response,\n",
    "                                             zenith,\n",
    "                                             azimuth,\n",
    "                                             dt,\n",
    "                                             n_hours,\n",
    "                                             binsize=6,\n",
    "                                             cut=90,\n",
    "                                             altitude_correction=False,\n",
    "                                             al=None):\n",
    "    \"\"\"\n",
    "    Get Compton response from hit pixel for each zenith/azimuth vector input.\n",
    "    Binsize determines regular sky coordinate grid in degrees.\n",
    "\n",
    "    :param: zenith        Zenith positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :param: azimuth       Azimuth positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :option: binsize      Default 5 deg (matching the sky dimension of the response). If set\n",
    "                          differently, make sure it matches the sky dimension as otherwise,\n",
    "                          false results may be returned\n",
    "    :option: cut          Threshold to cut the response calculation after a certain zenith angle.\n",
    "                          Default 60\n",
    "    :param: n_hours       Number of hours in cdxervation\n",
    "    :option: altitude_correction Default False: use interpolated transmission probability, normalised to 33 km and 500 keV,\n",
    "                          to modify number of expected photons as a function of altitude and zenith angle of cdxervation\n",
    "    :option: al           Altitude values according to dt from construct_pointings(); used of altitude_correction is set to True\n",
    "    \"\"\"\n",
    "\n",
    "    # Azimuthal angle is periodic in the range [0,360]\n",
    "    # Zenith angle ranges from [0,180]\n",
    "\n",
    "    # Check which pixel (index) was hit on regular grid\n",
    "    hit_pixel_zi = np.floor(zenith/binsize)\n",
    "    hit_pixel_ai = np.floor(azimuth/binsize)\n",
    "\n",
    "    # Check which pixel centre\n",
    "    hit_pixel_z = (hit_pixel_zi+0.5)*binsize\n",
    "    hit_pixel_a = (hit_pixel_ai+0.5)*binsize\n",
    "\n",
    "    # Check which zeniths are beyond threshold\n",
    "    bad_idx = np.where(hit_pixel_z > cut)\n",
    "\n",
    "    # Choose pixels with hits to write to output array\n",
    "    za_idx = np.array([hit_pixel_zi,hit_pixel_ai]).astype(int)\n",
    "\n",
    "    nz = zenith.shape[2]\n",
    "\n",
    "    n_lon = int(360/binsize)\n",
    "    n_lat = int(180/binsize)\n",
    "    \n",
    "    l_arrg = np.linspace(-180,180,int(360/binsize)+1)\n",
    "    b_arrg = np.linspace(-90,90,int(180/binsize)+1)\n",
    "    L_ARRg, B_ARRg = np.meshgrid(l_arrg,b_arrg)\n",
    "    l_arr = l_arrg[0:-1]+binsize/2\n",
    "    b_arr = b_arrg[0:-1]+binsize/2\n",
    "    L_ARR, B_ARR = np.meshgrid(l_arr,b_arr)\n",
    "\n",
    "    # Take care of regular grid by applying weighting with latitude\n",
    "    weights = ((binsize*np.pi/180)*(np.sin(np.deg2rad(B_ARR+binsize/2)) - np.sin(np.deg2rad(B_ARR-binsize/2)))).repeat(nz).reshape(n_lat,n_lon,nz)\n",
    "    weights[bad_idx] = 0\n",
    "\n",
    "    \n",
    "    # Check for negative weights and indices and remove them\n",
    "    weights[za_idx[0,:] < 0] = 0.\n",
    "    weights[za_idx[1,:] < 0] = 0.\n",
    "    za_idx[0,za_idx[0,:] < 0] = 0.\n",
    "    za_idx[1,za_idx[1,:] < 0] = 0.\n",
    "    \n",
    "    \n",
    "    if altitude_correction == True:\n",
    "        altitude_response = return_altitude_response()\n",
    "    else:\n",
    "        altitude_response = one_func\n",
    "\n",
    "    # Get responses for each pixel    \n",
    "    image_response = np.zeros((n_hours,n_lat,n_lon,Response.shape[2]))\n",
    "\n",
    "    for c in tqdm(range(n_hours)):\n",
    "        cdx = np.where((pointing.cdtpoins > analysis.dataset.times.times_min[analysis.dataset.times.n_ph_dx[c]]) &\n",
    "                       (pointing.cdtpoins <= analysis.dataset.times.times_max[analysis.dataset.times.n_ph_dx[c]]))[0]\n",
    "        \n",
    "        # This calculation is a look-up of the response entries. \n",
    "        image_response[c,:,:,:] += np.sum(Response[za_idx[0,:,:,cdx],za_idx[1,:,:,cdx],:]*np.einsum('klij->iklj', weights[:,:,cdx,None])*dt[cdx,None,None,None],axis=0)\n",
    "\n",
    "    return image_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Translating the sponse in the Compton Data Space coordinates to the sky</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_CDS = rsp.rsp.response_grid_normed_efinal.reshape(\n",
    "    n_b,\n",
    "    n_l,\n",
    "    analysis.dataset.phis.n_phi_bins*\\\n",
    "    analysis.dataset.fisbels.n_fisbel_bins,\n",
    "    10)[:,:,nonzero_idx,ebin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced response dimensions:\n",
    "# lat x lon x CDS\n",
    "sky_response_CDS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculation of the general response for the current data set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_scaled = get_image_response_from_pixelhit_general(\n",
    "    Response=sky_response_CDS,\n",
    "    zenith=zensgrid,\n",
    "    azimuth=azisgrid,\n",
    "    dt=pointing.dtpoins,\n",
    "    n_hours=analysis.dataset.times.n_ph,\n",
    "    binsize=6.,\n",
    "    #cut=90.,\n",
    "    altitude_correction=False,\n",
    "    al=np.ones(len(pointing.dtpoins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-set-specific response dimensions\n",
    "# times x lat x lon x CDS\n",
    "\n",
    "print('The data specific sky response dimensions:',sky_response_scaled.shape)\n",
    "sky_response_scaled_time = sky_response_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculation of the 'exposure map' - the response weighted by time:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expo_map_crab = np.zeros((n_b,n_l))\n",
    "for i in tqdm(range(sky_response_scaled_time)):\n",
    "    expo_map_crab += np.sum(sky_response_scaled[i,:,:,:],axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Choosing region of bad exposure - not seen by the instrument during the selected observation time:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_expo = np.where(expo_map_crab/domega <= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting the exposure map weighted with the pixel size:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.style.use('default')\n",
    "plt.subplot(projection='aitoff')\n",
    "plt.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,np.flip(expo_map_crab/domega,axis=1))\n",
    "plt.colorbar(orientation='horizontal')\n",
    "cont1 = plt.contour(L_ARR*deg2rad,B_ARR*deg2rad,np.flip(expo_map_crab/domega,axis=1),colors='green')\n",
    "cont2 = plt.contour(L_ARR*deg2rad,B_ARR*deg2rad, np.flip(expo_map_crab/domega,axis=1), levels =[0],colors='r')\n",
    "plt.xticks(np.array([-150,-120,-90,-60,-30,0,30,60,90,120,150])*deg2rad,labels=[r'$150^{\\circ}$'+'\\n',\n",
    "                                                         r'$120^{\\circ}$'+'\\n',\n",
    "                                                         r'$90^{\\circ}$'+'\\n',\n",
    "                                                         r'$60^{\\circ}$'+'\\n',\n",
    "                                                         r'$30^{\\circ}$'+'\\n',                       \n",
    "                                                         r'$0^{\\circ}$'+'\\n',\n",
    "                                                         r'$330^{\\circ}$'+'\\n',\n",
    "                                                         r'$300^{\\circ}$'+'\\n',                       \n",
    "                                                         r'$270^{\\circ}$'+'\\n',\n",
    "                                                         r'$240^{\\circ}$'+'\\n',                                                    \n",
    "                                                         r'$210^{\\circ}$'+'\\n'],color='lightgray')\n",
    "\n",
    "plt.scatter(np.deg2rad(360-l1),np.deg2rad(b1),color='aquamarine',marker='.',s=500, label= 'Crab')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Begin Richardson-Lucy Deconvolution:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define function for a starting map fo the RL deconvolution - an isotropic map:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsoMap(ll,bb,A0,binsize=6):\n",
    "    shape = np.ones(ll.shape)\n",
    "    norm = np.sum(shape*(binsize*np.pi/180)*(np.sin(np.deg2rad(bb+binsize/2)) - np.sin(np.deg2rad(bb-binsize/2))))\n",
    "    val = A0*shape/norm\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define suitable background time nodes:</h3>\n",
    "We select background time nodes generated from a Baysian Blocks analysis to determine optimal sections of data where each background section if fit independently.\n",
    "For this imaging demonstration, we provide the optimal time nodes for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected background time notes [time bin]\n",
    "all_edges = np.array([  0 ,   6,  12,  23, 35, 48,  63,  79,  93,\n",
    "                       107, 121, 134, 149, 163, 176, 187, 199, 209,\n",
    "                       221, 231, 243, 255, 269, 281, 293, 308])\n",
    "all_edges_num = all_edges.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.step(np.arange(len(np.sum(analysis.dataset.binned_data,axis=(1,2,3)))),np.sum(analysis.dataset.binned_data,axis=(1,2,3)))\n",
    "for e in all_edges:\n",
    "    plt.axvline(e,linestyle='-', linewidth=0.5)\n",
    "    plt.title(\"Baysian Analysis Background Time Cuts\")\n",
    "    plt.xlabel(\"Time Bins [30min/bin]\")\n",
    "    plt.ylabel(\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Add these time nodes to the current background model:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background.make_bg_cuts(list(all_edges+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Select only the previously defined energy bin for data set:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crab_dataset = analysis.dataset.binned_data[:,ebin,:,:].reshape(sky_response_scaled_time,30*1145)[:,nonzero_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Select this energy bin for the background model and setting the cut values, indices, and total number of background cuts applied:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = background.bg_model_reduced[ebin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_cuts, idx_arr, Ncuts = background.bg_cuts, background.idx_arr, background.Ncuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set the initial background fits in the Richardson-Lucy algorithm to 1 for the first iteration, then to random values between 0.83 and 0.93 for the following iteration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bg_it1 = np.full(np.shape(all_edges), 1.0)\n",
    "fitted_bg = 0.85+np.random.rand(all_edges_num)*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Richardson-Lucy algorithm (individual step explaination included in code):</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial map (isotropic flat, small value)\n",
    "map_init = IsoMap(L_ARR,B_ARR,1)\n",
    "\n",
    "# Define background \n",
    "bg_cuts, idx_arr, Ncuts = background.bg_cuts, background.idx_arr, background.Ncuts\n",
    "\n",
    "# Number of RL iterations (100 iterations is recommended)\n",
    "iterations = 10\n",
    "\n",
    "# Scalling factor for the 'Delta map', values of either 1000 or 2000 are recommended\n",
    "afl_scl = 1000.\n",
    "\n",
    "# Below are several initialise arrays to save images and other parameters to maps per iteration\n",
    "map_iterations = np.zeros((n_b,n_l,iterations))\n",
    "# Likelihood of maps (vs. intitial map which is fully background)\n",
    "map_likelihoods = np.zeros(iterations)\n",
    "# Fit likelihoods\n",
    "intermediate_lp = np.zeros(iterations)\n",
    "# Acceleration parameters\n",
    "acc_par = np.zeros(iterations)\n",
    "# Background parameters\n",
    "bg_pars = np.zeros((iterations,Ncuts))\n",
    "\n",
    "# As the zeroth iteration, copy initial map to become the 'old map' (see below)\n",
    "map_old = map_init\n",
    "# cf. Knoedlseder+1997 what the values denominator etc are\n",
    "denominator = expo_map_crab\n",
    "\n",
    "# The zeroth iteration is the initial map\n",
    "map_iterations[:,:,0] = map_old\n",
    "\n",
    "# Convolve this map with the response\n",
    "expectation_init = 0\n",
    "print('Convolving with response (init expectation), iteration 0')\n",
    "for i in tqdm(range(n_b)):\n",
    "    for j in range(n_l):\n",
    "        expectation_init += sky_response_scaled[:,i,j,:]*map_init[i,j]\n",
    "\n",
    "# Set the old expectation (in data space bins) to new expectation (convolved image)\n",
    "expectation_old = expectation_init\n",
    "\n",
    "# Run the modified RL algorithm for the number of iterations defined above\n",
    "for its in tqdm(range(1,iterations)):\n",
    "    \n",
    "    # Setting the map to zero where we selected a bad exposure \n",
    "    map_old[bad_expo[0],bad_expo[1]] = 0\n",
    "    # Check for each pixel to be finite\n",
    "    map_old[np.where(np.isnan(map_old) == True)] = 0\n",
    "    \n",
    "    # Make new background for the next iteration\n",
    "    bg_cuts, idx_arr, Ncuts = background.bg_cuts, background.idx_arr, background.Ncuts\n",
    "    \n",
    "    # Temporary background model\n",
    "    tmp_model_bg = np.zeros((sky_response_scaled_time,background.bg_model_reduced[ebin].shape[1]))\n",
    "    \n",
    "    # Set the guess to the first background fit to 1 and set later fits to the random value of fitted_bg\n",
    "    if its == 1:\n",
    "        for g in range(sky_response_scaled_time):\n",
    "            tmp_model_bg[g,:] = background_model[g,:]*fitted_bg_it1[idx_arr-1][g]\n",
    "    else:\n",
    "        for g in range(sky_response_scaled_time):\n",
    "            tmp_model_bg[g,:] = background_model[g,:]*fitted_bg[idx_arr-1][g]\n",
    "\n",
    "\n",
    "            \n",
    "    # Expectation (in data space) is the image (expectation_old) plus the background (tmp_model_bg)\n",
    "    expectation_tot_old = expectation_old + tmp_model_bg \n",
    "\n",
    "    # Calculate likelihood of currect total expectation\n",
    "    map_likelihoods[its-1] = cashstat(crab_dataset.ravel(),expectation_tot_old.ravel())\n",
    "    \n",
    "    # Calculate numerator of RL algorithm\n",
    "    numerator = 0\n",
    "    print('Calculating Delta image, iteration '+str(its)+', numerator')\n",
    "    for i in tqdm(range(sky_response_scaled_time)):\n",
    "        for j in range(crab_dataset.shape[1]):\n",
    "            numerator += (crab_dataset[i,j]/expectation_tot_old[i,j]-1)*sky_response_scaled[i,:,:,j]\n",
    "    \n",
    "    # Calculate delta map (denominator scaled by fourth root to avoid exposure edge effects)\n",
    "    delta_map_tot_old = (numerator/denominator)*map_old*(denominator)**0.25\n",
    "    \n",
    "    # Check again for finite values and zero our bad exposure regions\n",
    "    nan_idx = np.where(np.isnan(delta_map_tot_old) == 1)\n",
    "    delta_map_tot_old[nan_idx[0],nan_idx[1]] = 0\n",
    "    delta_map_tot_old[bad_expo[0],bad_expo[1]] = 0\n",
    "\n",
    "    # These plots are not required but allow us to see how the RL algorithm is doing\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(121)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(map_old,axis=1,shift=30))\n",
    "    plt.colorbar()\n",
    "    plt.title('expectation old')\n",
    "    plt.subplot(122)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(delta_map_tot_old,axis=1,shift=30))\n",
    "    plt.title('delta map')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    # Convolve delta image\n",
    "    print('Convolving Delta image, iteration '+str(its))\n",
    "    conv_delta_map_tot = 0\n",
    "    for i in tqdm(range(n_b)):\n",
    "        for j in range(n_l):\n",
    "            conv_delta_map_tot += sky_response_scaled[:,i,j,:]*delta_map_tot_old[i,j]\n",
    "    \n",
    "    # Find the maximum acceleration parameter to multiply with delta image\n",
    "    # This will result in total image being positive everywhere\n",
    "    print('Finding maximum acceleration parameter, iteration '+str(its))\n",
    "    try:\n",
    "        len_arr = []\n",
    "        for i in range(0,10000):\n",
    "            len_arr.append(len(np.where((map_old+delta_map_tot_old*i/afl_scl) < 0)[0]))\n",
    "        len_arr = np.array(len_arr)\n",
    "        afl = np.max(np.where(len_arr == 0)[0])\n",
    "        print('Maximum acceleration parameter found: ',afl/afl_scl)\n",
    "\n",
    "        \n",
    "        # Fit delta map and current map to speed up RL algorithm\n",
    "        print('Fitting delta-map in addition to old map, iteration '+str(its))\n",
    "        # Dictionary for data set and prior\n",
    "        data_multimap = dict(N = nonzero_idx_num,\n",
    "                     Nh = sky_response_scaled_time,\n",
    "                     Ncuts = Ncuts,\n",
    "                     Nsky = 2,\n",
    "                     acceleration_factor_limit=afl*0.95,\n",
    "                     bg_cuts = bg_cuts,\n",
    "                     bg_idx_arr = idx_arr,\n",
    "                     y = crab_dataset.ravel().astype(int),\n",
    "                     bg_model = tmp_model_bg,\n",
    "                     conv_sky = np.concatenate([[expectation_old],[conv_delta_map_tot/afl_scl]]),\n",
    "                     mu_flux = np.array([1,afl/2]),\n",
    "                     sigma_flux = np.array([1e-2,afl]),\n",
    "                     mu_Abg = fitted_bg,\n",
    "                     sigma_Abg = np.repeat(0.1,Ncuts))\n",
    "\n",
    "        # Fit;\n",
    "        # Initial values for fit\n",
    "        init = {}\n",
    "        init['flux'] = np.array([1.,afl/2.])\n",
    "        init['Abg'] = np.repeat(0.99,Ncuts)\n",
    "        op2D = model_multimap.optimizing(data=data_multimap,init=init,as_vector=False,verbose=True,\n",
    "                                                tol_rel_grad=1e3,tol_obj=1e-20)\n",
    "\n",
    "        # Save values\n",
    "        print('Saving new map, and fitted parameters, iteration '+str(its))\n",
    "        intermediate_lp[its-1] = op2D['value']\n",
    "        acc_par[its-1] = op2D['par']['flux'][1]\n",
    "        bg_pars[its-1,:] = op2D['par']['Abg']\n",
    "  \n",
    "        # Make new map as old map plus scaled delta map\n",
    "        map_new = map_old+op2D['par']['flux'][1]*delta_map_tot_old/afl_scl\n",
    "    \n",
    "        # Do the same with the expectation (data space)\n",
    "        expectation_new = expectation_old + op2D['par']['flux'][1]*conv_delta_map_tot/afl_scl\n",
    "        \n",
    "    except:\n",
    "        # If the fit failed...\n",
    "        print('Proceeding without acceleration parameter.')\n",
    "        map_new = map_old+delta_map_tot_old\n",
    "        expectation_new = expectation_old + conv_delta_map_tot\n",
    "    \n",
    "    # Check finite values again\n",
    "    if its == 1:\n",
    "        bad_index_init = np.where(np.isnan(map_new) == True)\n",
    "    \n",
    "    # Also check that these values are zero\n",
    "    map_new[bad_expo[0],bad_expo[1]] = 0\n",
    "    map_new[np.where(np.isnan(map_new) == True)] = 0\n",
    "    map_iterations[:,:,its] = map_new\n",
    "\n",
    "    # Swap maps\n",
    "    map_old = map_new\n",
    "    \n",
    "    # Also swap expectations\n",
    "    expectation_old = expectation_new\n",
    "    \n",
    "    \n",
    "    # Repeat for each iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now we can extract/plot some results:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the significance of the last iteration\n",
    "siglist = np.abs(map_likelihoods-map_likelihoods.max())\n",
    "significance = math.sqrt(siglist[-2])\n",
    "print(\"The significance (sigma) of this image is, \", significance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculating the fluxes of the <em>total</em> maps<br>\n",
    "    If flux at position is to be calculated, select central pixel and the neighbouring pixels around that.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_fluxesA = np.zeros(iterations)\n",
    "for i in range(iterations):\n",
    "    map_fluxesA[i] = np.sum(map_iterations[:,:,i]*domega)\n",
    "print('Crab flux in ph/keV for this energy bin: ', map_fluxesA[iterations-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_fluxesB = np.zeros(iterations)\n",
    "for i in range(iterations):\n",
    "    #map_iterations_nan[:,:,iterations-1]/domega/analysis.dataset.times.total_time*(DeltaE)\n",
    "    map_fluxesB[i] = np.sum(map_iterations[:,:,i-1]/analysis.dataset.times.total_time*(DeltaE))\n",
    "print('Crab flux in ph/cm^2/s for this energy bin: ', map_fluxesB[iterations-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting the likelihood ratio test of all the iterations:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(map_likelihoods-map_likelihoods.max()),'o-')\n",
    "plt.xlim(0,iterations-2 )\n",
    "plt.ylim(100,750)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Test Statistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Plotting the flux of the image per iteration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(map_fluxesA,'o-')\n",
    "plt.xlim(0,iterations)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Flux [ph/keV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot a map in galactic coordinates:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_iterations_nan = np.copy(map_iterations)\n",
    "\n",
    "plt.figure(figsize=(10.24,7.68))\n",
    "\n",
    "# bad exposure regions will be gray\n",
    "cmap = plt.get_cmap('viridis')\n",
    "cmap.set_bad('lightgray')\n",
    "map_iterations_nan = np.copy(map_iterations)\n",
    "bad_expo = np.where(expo_map_crab/domega < 100)\n",
    "for i in range(iterations):\n",
    "    map_iterations_nan[bad_expo[0],bad_expo[1],i] = np.nan\n",
    "\n",
    "plt.subplot(projection='aitoff')\n",
    "plt.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,\n",
    "               np.flip(map_iterations_nan[:,:,iterations-1]/domega,axis=1)/analysis.dataset.times.total_time*(DeltaE),\n",
    "               cmap=cmap,\n",
    "               norm=colors.PowerNorm(1),\n",
    "               rasterized=True)\n",
    "                \n",
    "plt.xticks(np.array([-150,-120,-90,-60,-30,0,30,60,90,120,150])*deg2rad,labels=[r'$150^{\\circ}$'+'\\n',\n",
    "                                                         r'$120^{\\circ}$'+'\\n',\n",
    "                                                         r'$90^{\\circ}$'+'\\n',\n",
    "                                                         r'$60^{\\circ}$'+'\\n',\n",
    "                                                         r'$30^{\\circ}$'+'\\n',                       \n",
    "                                                         r'$0^{\\circ}$'+'\\n',\n",
    "                                                         r'$330^{\\circ}$'+'\\n',\n",
    "                                                         r'$300^{\\circ}$'+'\\n',                       \n",
    "                                                         r'$270^{\\circ}$'+'\\n',\n",
    "                                                         r'$240^{\\circ}$'+'\\n',                                                    \n",
    "                                                         r'$210^{\\circ}$'+'\\n'],color='lightgray')\n",
    "\n",
    "plt.yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "\n",
    "plt.xlabel('Gal. Lon. [deg]')\n",
    "plt.ylabel('Gal. Lat. [deg]')\n",
    "plt.grid()\n",
    "cbar = plt.colorbar(orientation='horizontal', shrink=0.8)\n",
    "cbar.set_label('Flux [ph cm$^{-2}$ s$^{-1}$]')\n",
    "\n",
    "plt.scatter(np.deg2rad(360-l1),np.deg2rad(b1),color='aqua',marker='*',s=200, label= 'Crab')\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
