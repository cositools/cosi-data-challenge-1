{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to $^{26}$Al imaging with cosipy classic\n",
    "In this notebook, we'll use a Richardson-Lucy deconvolution algorithm to image $^{26}$Al emission from the Milky Way Galaxy. As MeV gamma-ray instruments, COSI-balloon and the COSI satellite are uniquely equipped to study the 1.809 MeV signature emission from this radioisotope, which traces stellar nucleosynthesis over millions of years. \n",
    "\n",
    "In the 1980s, NASA's High Energy Astrophysical Observatory (HEAO-3) satellite mission detected 1.809 MeV emission emanating from the direction of the Galactic Center [Mahoney et al. 1984](https://adsabs.harvard.edu/pdf/1984ApJ...286..578M). This marked the discovery of Galactic $^{26}$Al. The spectrum is shown below.\n",
    "\n",
    "![HEAO-3 26Al spectrum (Mahoney et al. 1984)](Mahoney_1984_HEAO-3_1809keV.png)\n",
    "\n",
    "\n",
    "Subsequent observations by the Compton telescope (COMPTEL) on-board NASA's Compton Gamma Ray Observatory (CGRO) yielded the first image of $^{26}$Al emission ([Oberlack et al. 1996](https://ui.adsabs.harvard.edu/abs/1996A%26AS..120C.311O/abstract), [Oberlack 1997](https://ui.adsabs.harvard.edu/abs/1997PhDT.........8O/abstract), [Pluschke et al. 2001](https://ui.adsabs.harvard.edu/abs/2001ESASP.459...55P/abstract)). Emission is concentrated in the Inner Galaxy ($|\\ell| \\leq 30^{\\circ}, |b| \\leq 10^{\\circ}$) with enhanced emission in regions of massive star activity, including Cygnus, Carina, and Vela. \n",
    "\n",
    "The SPectrometer on INTEGRAL (SPI) largely corroborated the features seen in the COMPTEL image with over a decade of observation time from ESA's INTEGRAL satellite ([Bouchet et al. 2015](https://iopscience.iop.org/article/10.1088/0004-637X/801/2/142/meta)). Emission is concentrated in the Inner Galaxy with a reported flux of $\\sim 3.3 \\times 10^{-4}$ ph cm$^{-2}$ s$^{-1}$. As in the COMPTEL image, there is enhanced emission in regions of massive star activity, including Perseus/Taurus, Cygnus/Cepheus, Carina, Vela, and Scorpius-Centaurus. \n",
    "\n",
    "\n",
    "COMPTEL 1.8 MeV image (Pluschke et al. 2001) | SPI 1.8 MeV image (Bouchet et al. 2015)\n",
    "- | -\n",
    "![COMPTEL 1.8 MeV image](COMPTEL_1.8MeV_image.png) | ![SPI 1.8 MeV image](SPI_1.8MeV_image.png)\n",
    "\n",
    "\n",
    "\n",
    "The COSI-balloon flight in 2016 measure the 1.809 MeV signature of $^{26}$Al with $3.7\\sigma$ significance, corresponding to about 106 $^{26}$Al photons [(Beechert et al. 2022)](https://iopscience.iop.org/article/10.3847/1538-4357/ac56dc/meta). The reported Inner Galaxy flux of $(8.6 \\pm 2.5) \\times 10^{-4}$ ph cm$^{-2}$ s$^{-1}$ and line centroid of $1811.2 \\pm 1.8$ keV are consistent with results from SPI and COMPTEL within 2$\\sigma$ uncertainties. Future observations with the COSI satellite (significantly increased observation time, greater effective area at 1.8 MeV, better constraints on high-latitude emission, and finer angular resolution) will comprise an important comparison to the balloon measurement, which was the first measurement of $^{26}$Al on a compact Compton telescope.\n",
    "\n",
    "Furthermore, the COSI satellite's full-sky observations with fine angular resolution have potential to more closely study individual regions of massive star activity; in particular, resolving individual sites of emission within Cygnus is a promising goal of the mission. Detailed imaging and spectrosocpic studies of the region may inform better understanding of the dynamics of $^{26}$Al after it is produced and ejected from massive stars. \n",
    "\n",
    "In this notebook, you will image the Galactic $^{26}$Al emission (traced by the DIRBE 240$\\mu$m image) as seen during the COSI-balloon flight in 2016. The flux is simulated at 10X the observed $^{26}$Al flux (10X Inner Galaxy flux = $3.3 \\times 10^{-3}$ ph cm$^{-2}$ s$^{-1}$; 10X total map flux = $1.2 \\times 10^{-2}$ ph cm$^{-2}$ s$^{-1}$) for robust statistics. You should expect to see extended emission along the Galactic Plane, similar to that revealed by COMPTEL and SPI's 1.8 MeV images. The massive star regions of Cygnus, Carina, and Vela will not be as easily identifiable in this simulation of the balloon flight; for those, be sure to participate in the data challenge (and real data analysis) of the COSI satellite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to make sure you're in the python environment you configured for COSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "We'll need functions from COSIpy \"classic.\"\n",
    "\n",
    "We also need some other standard Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from COSIpy_dc1 import *\n",
    "import response_dc1\n",
    "from COSIpy_tools_dc1 import *\n",
    "\n",
    "import pickle\n",
    "import pystan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the path to your data/simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/jacqueline/COSItools/cosi-data-challenge-1/data_products'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter the file name of the simulation to be imaged\n",
    "This file contains the 10X flux 26Al simulation and Ling BG. \n",
    "\n",
    "You can optionally image only 26Al (without background) by changing this file to the 26Al-only simulation. You will have to adjust the RL algorithm parameters later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al-26 (with Ling BG)\n",
    "tra = 'DC1_Al26_10xFlux_and_Ling.inc1.id1.extracted.tra.gz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the main cosipy \"analysis1\" object, which provides various functionalities to study the specified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1 = COSIpy(data_dir, tra)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the file. This could take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1.read_COSI_DataSet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin the data and generate the light curve and energy spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin the data in time. \n",
    "The COSI-balloon instrument freely floated on the balloon platform. This means that, unlike a space or ground-based telescope with well-defined pointings and slewing schedule, its orientation was largely dependent on the unconstrained path of the balloon. It was a zenith-pointing instrument, meaning that its vertical orientation pointed straight above the hanging instrument, towards the balloon above it.\n",
    "\n",
    "The exception to this freedom is that during the day time, COSI's azimuthal orientation was fixed such that its solar panels remained oriented facing the Sun. At nighttime, though, the instrument freely rotated about its azimuth. \n",
    "\n",
    "This is all to say that COSI's orientation (e.g. roll/pitch/yaw) changed rapidly during flight. As such, we might prefer to bin the data into very small (~seconds) time bins to preserve an accurate orientation of the instrument tied to the data. However, this would require massive computational resources. Also, time bins which are too small may contain too few photons for meaningful analysis. \n",
    "\n",
    "Through extensive testing, **1800 second** (30 minute) time bins were found to strike a practical balance between a sufficiently precise treatment of instrument orientation and computational means. \n",
    "\n",
    "You can feel free to play with the time binning. You may choose to decrease the size to 900 s, or increase it to 3600 s, for example, to see if there's an effect on the image (e.g. does it look less/more blurred, respectively, as you lump more data into more/fewer time bins?)\n",
    "\n",
    "You can also manually define time bins as an array of time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time bins of equal width \"Delta_T\"\n",
    "# You can change Delta_T, though keep in mind that making it too small could use too much memory!! \n",
    "Delta_T = 1800 # s\n",
    "analysis1.dataset.time_binning_tags(time_bin_size=Delta_T)\n",
    "\n",
    "# Or, manually define time bins by time stamps. For example,\n",
    "# time_bin_edges_trunc = np.arange(min(times), max(times)-86400, Delta_T)\n",
    "# print(time_bin_edges_trunc)\n",
    "# print(time_bin_edges_trunc[-2], time_bin_edges_trunc[-1], '\\n')\n",
    "# analysis1.dataset.time_binning_tags_2(time_bin_edges_trunc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin the data in energy.\n",
    "Define the energy bins exactly as they are defined in the response. \n",
    "\n",
    "For 26Al, we use a response simulation with only one energy bin around the 1809 keV photopeak signature: **1803-1817 keV**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of energy bins (same as the response)\n",
    "energy_bin_edges = np.array([1803, 1817])\n",
    "\n",
    "energy_bin_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the size of pixels in the sky.\n",
    "As with the energy binning, the pixel size here must match that of the response. The responses that have been simulated for COSI-balloon assume $6^{\\circ} \\times 6^{\\circ}$ resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_size = 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate the binning and bin the data.\n",
    "Calling \"get_binned_data()\" may take several minutes, depending on the size of the dataset and the number of bins. Keep an eye on memory here: if your time bins are very small, for example, this could be an expensive operation.\n",
    "\n",
    "As running, \"get_binned_data()\" uses about 12 GB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the binning\n",
    "analysis1.dataset.init_binning(energy_bin_edges=energy_bin_edges, pixel_size=pixel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin data\n",
    "analysis1.dataset.get_binned_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the shape of the binned data.\n",
    "The binned data are contained in \"analysis1.dataset.binned_data.\" This is a 4-dimensional object: \\\n",
    "(time, energy, $\\phi$, FISBEL)\n",
    "\n",
    "The number of bins in each dimension are shown by calling \"shape.\"\n",
    "\n",
    "Per the binning definitions above, there are 365 time bins, 10 energy bins (as governed by those in the response), 30 $\\phi$ bins ($\\phi$ is the Compton scattering angle; 30 bins of $6^{\\circ}$ spanning the full $0-180^{\\circ}$ range of possible Compton scattering angles), and 1145 FISBEL bins. \n",
    "\n",
    "FISBEL is a unique index which specifies the $\\chi$ and $\\psi$ dimensions of the Compton Data Space (CDS), whose third dimension is $\\phi$. The $\\chi$ and $\\psi$ dimensions specify the direction of the scattered photon in a Compton interaction. \n",
    "\n",
    "How do we end up with 1145 FISBEL bins? Consider a sphere which is $4 \\pi( 180^{\\circ}/ \\pi)^2 = 41252.96 \\textrm{ deg}^{2}$ \\\n",
    "Given our $6 \\textrm{ deg}^{2}$ binning, we have $41252.96 \\textrm{ deg}^{2}$ / $6 \\textrm{ deg}^{2}$ $\\sim$ 1145 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time, energy, phi, fisbel\")\n",
    "print(analysis1.dataset.binned_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can print the number of time bins, the width of each time bin, and the total time\n",
    "print(analysis1.dataset.times.n_time_bins)\n",
    "print(analysis1.dataset.times.times_wid)\n",
    "print(analysis1.dataset.times.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get a lightcurve, we integrate over all energies and the entire CDS ($\\phi$ + FISBEL). In python, this is a sum over the latter three dimensions of the binned_data object, i.e. everything except time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing over energy, phi and fisbel. Everything except the time axis\n",
    "binned_counts = np.sum(analysis1.dataset.binned_data, axis=(1, 2, 3))\n",
    "\n",
    "print(f'Number of counts: {np.sum(binned_counts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_bins = analysis1.dataset.binned_data.shape[0]\n",
    "print(f'Number of time bins: {n_time_bins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1.dataset.n_time_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightcurve\n",
    "plt.step(np.arange(n_time_bins), binned_counts, where='mid', color='k')\n",
    "\n",
    "plt.xlabel(f'Time bin [DeltaT = {Delta_T} s]')\n",
    "plt.ylabel('Counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can plot the energy spectrum directly from the cosipy analysis object.\n",
    "For a single energy bin, the spectrum is necessarily a top hat in the sole non-zero bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1.dataset.plot_raw_spectrum()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pointing object with the cosipy pointing class.\n",
    "This may also take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of poitings (balloon stability + Earth rotation)\n",
    "pointing1 = Pointing(dataset=analysis1.dataset,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the BG model. \n",
    "We model the background using extensive simulations of Earth's atmospheric $\\gamma$-ray background. The simulations assume the Ling model of atmospheric $\\gamma$-ray emission ([Ling 1975](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/JA080i022p03241), which is often adopted for this purpose in MeV $\\gamma$-ray astrophysics experiments.\n",
    "\n",
    "The simulations use an accurate mass model of the COSI-balloon instrument during flight and follow the true orientation of the instrument as it traveled along its flight path.\n",
    "\n",
    "Notably, this background model excludes the significant background from instrumental activation. Instrumental activation refers to the excitation of instrument materials by bombarding high-energy particles, e.g. cosmic rays. The instrument materials subsequently de-excite via the emission of $\\gamma$-rays which fall in COSI's energy bandpass. Often, the de-excitation lines exactly overlap with astrophysical lines of interest, including 511 keV and 1809 keV. \n",
    "\n",
    "A complete treatment of the background, therefore, would include both atmospheric and instrumental activation simulations. For simplicity, however, in this imaging tutorial we model only atmospheric background. Future data challenges will include instrumental activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ling BG simulation to model atmospheric background\n",
    "background1 = BG(dataset=analysis1.dataset,\n",
    "                mode='from file',\n",
    "                filename='/home/jacqueline/COSItools/cosi-data-challenge-1/data_products/Scaled_Ling_BG_1x.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26Al response\n",
    "rsp = response.SkyResponse(\n",
    "   filename='/home/jacqueline/COSItools/cosi-data-challenge-1/data_products/1809keV_imaging_response.npz', pixel_size=pixel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The shape of the response spans (Galactic latitude $b$, Galactic longitude $\\ell$, Compton scattering angle $\\phi$,  FISBEL, energy).\n",
    "\n",
    "The size of each dimension depends on the chosen pixel size. Here, we've chosen $6^{\\circ}$ pixels. \n",
    "\n",
    "Galactic latitude $b \\in [-90^{\\circ}, 90^{\\circ}] \\rightarrow$ 30 bins.\\\n",
    "Galactic longitude $\\ell \\in [-180^{\\circ}, 180^{\\circ}] \\rightarrow$ 60 bins.\\\n",
    "Compton scattering angle $\\phi \\in [0^{\\circ}, 180^{\\circ}] \\rightarrow$ 30 bins (\"analysis1.dataset.phis.n_phi_bins\").\\\n",
    "See above for explanation of 1145 FISBEL bins (\"rsp.rsp.n_fisbel_bins\").\\\n",
    "There is 1 energy bin in the 1809 keV response (\"analysis1.dataset.energies.n_energy_bins\"). This is why there is no fifth dimension for energy printed below. When using the continuum response (see point source imaging notebook), which has 10 energy bins, this cell prints (30, 60, 30, 1145, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp.rsp.response_grid_normed_efinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The shape of the data and background objects span (time, Galactic latitude $b$, Galactic longitude $\\ell$, FISBEL).\n",
    "\n",
    "Given the time bin size \"Delta_T\" which we defined at the beginning of the notebook, there are 2242 time bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(analysis1.dataset.binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(background1.bg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate the response at $90^{\\circ}$. \n",
    "As discussed above, COSI's field of view extends $60^{\\circ}$ beyond its zenith. The data/simulations themselves, however, only have hard cut manually applied at $90^{\\circ}$: this is the Earth Horizon Cut (EHC). The EHC is applied to the data in order to remove background $\\gamma$-ray emanating from the Earth's atmosphere below the instrument. \n",
    "\n",
    "To preserve any photons which may scatter in just beyond COSI's nominal $60^{\\circ}$ field of view but not beyond the $90^{\\circ}$ EHC, we define a \"cut\" which trucates, i.e. zeroes out, the response at $90^{\\circ}$. \n",
    "\n",
    "Note that because the EHC has removed all photons beyond $90^{\\circ}$, setting the cut to a value greater than $90^{\\circ}$ will behave identically to cut = $90^{\\circ}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an energy bin to analyze.\n",
    "We only have one energy bin (1803-1817 keV), so the index is necessarily 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebin = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a grid on the sky to make images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenient variable for deg --> radian conversion\n",
    "deg2rad = np.pi/180.\n",
    "\n",
    "# We define our sky-grid on a regular (pixel_size x pixel_size) grid for testing (later finer grid)\n",
    "binsize = pixel_size\n",
    "\n",
    "# Galactic coordiantes: l and b pixel edges\n",
    "l_arrg = np.linspace(-180, 180, int(360/binsize)+1)\n",
    "b_arrg = np.linspace(-90, 90, int(180/binsize)+1)\n",
    "\n",
    "# Number of pixels in l and b\n",
    "n_l = int(360/binsize)\n",
    "n_b = int(180/binsize)\n",
    "\n",
    "# Making a grid\n",
    "L_ARRg, B_ARRg = np.meshgrid(l_arrg, b_arrg)\n",
    "\n",
    "# Choosing the centre points as representative\n",
    "l_arr = l_arrg[0:-1] + binsize/2\n",
    "b_arr = b_arrg[0:-1] + binsize/2\n",
    "L_ARR, B_ARR = np.meshgrid(l_arr, b_arr)\n",
    "\n",
    "# Define solid angle for each pixel for normalisations later\n",
    "domega = (binsize*deg2rad)*(np.sin(np.deg2rad(B_ARR + binsize/2)) - np.sin(np.deg2rad(B_ARR - binsize/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sky grid to zenith/azimuth pairs for all pointings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the zeniths and azimuths on that grid for all times\n",
    "zensgrid,azisgrid = zenaziGrid(pointing1.ypoins[:,0], pointing1.ypoins[:,1],\n",
    "                               pointing1.xpoins[:,0], pointing1.xpoins[:,1],\n",
    "                               pointing1.zpoins[:,0], pointing1.zpoins[:,1],\n",
    "                               L_ARR.ravel(), B_ARR.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for next routines ... \n",
    "zensgrid = zensgrid.reshape(n_b, n_l, len(pointing1.xpoins))\n",
    "azisgrid = azisgrid.reshape(n_b, n_l, len(pointing1.xpoins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the observation indices where we actually have measured photons (important for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_idx = background1.calc_this[ebin]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the response of an image for arbitrary time binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_response_from_pixelhit_general(Response,zenith,azimuth,dt,n_hours,binsize=6,cut=60,altitude_correction=False,al=None):\n",
    "    \"\"\"\n",
    "    Get Compton response from hit pixel for each zenith/azimuth vector(!) input.\n",
    "    Binsize determines regular(!!!) sky coordinate grid in degrees.\n",
    "\n",
    "    :param: zenith        Zenith positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :param: azimuth       Azimuth positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :option: binsize      Default 5 deg (matching the sky dimension of the response). If set\n",
    "                          differently, make sure it matches the sky dimension as otherwise,\n",
    "                          false results may be returned\n",
    "    :option: cut          Threshold to cut the response calculation after a certain zenith angle.\n",
    "                          Default 60\n",
    "    :param: n_hours       Number of hours in cdxervation\n",
    "    :option: altitude_correction Default False: use interpolated transmission probability, normalised to 33 km and 500 keV,\n",
    "                          to modify number of expected photons as a function of altitude and zenith angle of cdxervation\n",
    "    :option: al           Altitude values according to dt from construct_pointings(); used of altitude_correction is set to True\n",
    "    \"\"\"\n",
    "\n",
    "    # assuming useful input:\n",
    "    # azimuthal angle is periodic in the range [0,360[\n",
    "    # zenith ranges from [0,180[\n",
    "\n",
    "    # check which pixel (index) was hit on regular grid\n",
    "    hit_pixel_zi = np.floor(zenith/binsize)\n",
    "    hit_pixel_ai = np.floor(azimuth/binsize)\n",
    "\n",
    "    # and which pixel centre\n",
    "    hit_pixel_z = (hit_pixel_zi+0.5)*binsize\n",
    "    hit_pixel_a = (hit_pixel_ai+0.5)*binsize\n",
    "\n",
    "    # check which zeniths are beyond threshold\n",
    "    bad_idx = np.where(hit_pixel_z > cut)\n",
    "\n",
    "    # set hit pixels to output array\n",
    "    za_idx = np.array([hit_pixel_zi,hit_pixel_ai]).astype(int)\n",
    "\n",
    "    nz = zenith.shape[2]\n",
    "\n",
    "    n_lon = int(360/binsize)\n",
    "    n_lat = int(180/binsize)\n",
    "    \n",
    "    l_arrg = np.linspace(-180,180,int(360/binsize)+1)\n",
    "    b_arrg = np.linspace(-90,90,int(180/binsize)+1)\n",
    "    L_ARRg, B_ARRg = np.meshgrid(l_arrg,b_arrg)\n",
    "    l_arr = l_arrg[0:-1]+binsize/2\n",
    "    b_arr = b_arrg[0:-1]+binsize/2\n",
    "    L_ARR, B_ARR = np.meshgrid(l_arr,b_arr)\n",
    "\n",
    "    # take care of regular grid by applying weighting with latitude\n",
    "    weights = ((binsize*np.pi/180)*(np.sin(np.deg2rad(B_ARR+binsize/2)) - np.sin(np.deg2rad(B_ARR-binsize/2)))).repeat(nz).reshape(n_lat,n_lon,nz)\n",
    "    weights[bad_idx] = 0\n",
    "\n",
    "    \n",
    "    # check for negative weights and indices and remove\n",
    "    weights[za_idx[0,:] < 0] = 0.\n",
    "    weights[za_idx[1,:] < 0] = 0.\n",
    "    za_idx[0,za_idx[0,:] < 0] = 0.\n",
    "    za_idx[1,za_idx[1,:] < 0] = 0.\n",
    "        \n",
    "    \n",
    "    if altitude_correction == True:\n",
    "        altitude_response = return_altitude_response()\n",
    "    else:\n",
    "        altitude_response = one_func\n",
    "\n",
    "    # get responses at pixels    \n",
    "    image_response = np.zeros((n_hours,n_lat,n_lon,Response.shape[2]))\n",
    "\n",
    "    for c in tqdm(range(n_hours)):\n",
    "        cdx = np.where((pointing1.cdtpoins > analysis1.dataset.times.times_min[analysis1.dataset.times.n_ph_dx[c]]) &\n",
    "                       (pointing1.cdtpoins <= analysis1.dataset.times.times_max[analysis1.dataset.times.n_ph_dx[c]]))[0]\n",
    "    \n",
    "        # this calculation is basically a look-up of the response entries. In general, weighting (integration) with the true shape can be introduced, however with a lot more computation time (Simpson's rule in 2D ...)\n",
    "        image_response[c,:,:,:] += np.sum(Response[za_idx[0,:,:,cdx],za_idx[1,:,:,cdx],:]*np.einsum('klij->iklj', weights[:,:,cdx,None])*dt[cdx,None,None,None],axis=0)#*altitude_weights[:,:,None]\n",
    "        \n",
    "    return image_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce the response dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_CDS = rsp.rsp.response_grid_normed_efinal.reshape(\n",
    "    n_b,\n",
    "    n_l,\n",
    "    analysis1.dataset.phis.n_phi_bins*\\\n",
    "    analysis1.dataset.fisbels.n_fisbel_bins, analysis1.dataset.energies.n_energy_bins)[:, :, nonzero_idx, ebin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced response dimensions:\n",
    "# lat x lon x CDS\n",
    "sky_response_CDS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of the general response for the current data set.\n",
    "This has to be done only once (for the data set).\n",
    "\n",
    "Takes ~40 minutes to run and ~100 GB memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_scaled = get_image_response_from_pixelhit_general(\n",
    "    Response=sky_response_CDS,\n",
    "    zenith=zensgrid,\n",
    "    azimuth=azisgrid,\n",
    "    dt=pointing1.dtpoins,\n",
    "    n_hours=analysis1.dataset.times.n_ph,\n",
    "    binsize=pixel_size,\n",
    "    cut=cut,\n",
    "    altitude_correction=False,\n",
    "    al=np.ones(len(pointing1.dtpoins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-set-specific response dimensions\n",
    "# times x lat x lon x CDS\n",
    "sky_response_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of the \"exposure map,\" i.e. the response weighted by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expo_map = np.zeros((n_b, n_l))\n",
    "\n",
    "for i in tqdm(range(sky_response_scaled.shape[0])):\n",
    "    expo_map += np.sum(sky_response_scaled[i,:,:,:], axis=2)\n",
    "    \n",
    "print(f'expo_map shape: {expo_map.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the exposure map weighted with the pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(projection='aitoff')\n",
    "p = plt.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,np.roll(expo_map/domega,axis=1,shift=0))\n",
    "plt.contour(L_ARR*deg2rad,B_ARR*deg2rad,np.roll(expo_map/domega,axis=1,shift=0),colors='black')\n",
    "plt.colorbar(p, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging\n",
    "Set up the Richardson-Lucy deconvolution algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define regions of the sky that we actually cannot see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we select everything, i.e. we have no bad exposure\n",
    "\n",
    "bad_expo = np.where(expo_map/domega <= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for a starting map for the RL deconvolution. We choose an isotropic map, i.e. all pixels on the sky are initialized with the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsoMap(ll,bb,A0,binsize=pixel_size):\n",
    "    shape = np.ones(ll.shape)\n",
    "    norm = np.sum(shape*(binsize*np.pi/180)*(np.sin(np.deg2rad(bb+binsize/2)) - np.sin(np.deg2rad(bb-binsize/2))))\n",
    "    val = A0*shape/norm\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of time bins (should be the first dimension of the response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2h = n_time_bins \n",
    "d2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only one energy bin (as above) for data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ebin: ',ebin)\n",
    "dataset = analysis1.dataset.binned_data[:,ebin,:,:].reshape(d2h,\n",
    "                                                            analysis1.dataset.phis.n_phi_bins*analysis1.dataset.fisbels.n_fisbel_bins)[:,nonzero_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same for background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = background1.bg_model_reduced[ebin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency of data and background\n",
    "They must have the same dimensions. If not, the algorithm won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape, background_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define background model cuts, indices, and resulting number of cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the modified RL algorithm implemented here, we need to load a stan model that fits background plus two images (the current image plus a delta image given by the RL formalism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #read stanmodel.pkl (if already compiled)\n",
    "    model_multimap = pickle.load(open('stanmodel.pkl', 'rb'))\n",
    "\n",
    "except:\n",
    "    print('Model not yet compiled, doing that now (might take a while).')\n",
    "    \n",
    "    # compile model (if not yet compiled):\n",
    "    model_multimap = pystan.StanModel('stanmodel.stan')\n",
    "\n",
    "    with open(data_dir + 'stanmodel.pkl', 'wb') as f:\n",
    "        pickle.dump(model_multimap, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in contours from a real tracer of $^{26}$Al to plot over our RL image\n",
    "Read in the DIRBE 240$\\mu$m map (a far-infrared survey considered to be a good tracer of $^{26}$Al emission) as a FITS file, rebin it to $6^{\\circ} \\times 6^{\\circ}$ resolution, and plot contours over the RL iterations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_ndarray(ndarray, new_shape, operation=\"sum\"):\n",
    "    \"\"\"\n",
    "    Bins an ndarray in all axes based on the target shape, by summing or\n",
    "        averaging.\n",
    "    Number of output dimensions must match number of input dimensions.\n",
    "    Example\n",
    "    -------\n",
    "    >>> m = np.arange(0,100,1).reshape((10,10))\n",
    "    >>> n = bin_ndarray(m, new_shape=(5,5), operation=‘sum’)\n",
    "    >>> print(n)\n",
    "    [[ 22  30  38  46  54]\n",
    "     [102 110 118 126 134]\n",
    "     [182 190 198 206 214]\n",
    "     [262 270 278 286 294]\n",
    "     [342 350 358 366 374]]\n",
    "    \"\"\"\n",
    "    if not operation.lower() in ['sum', 'mean', 'average', 'avg']:\n",
    "        raise ValueError(\"Operation {} not supported.\".format(operation))\n",
    "    if ndarray.ndim != len(new_shape):\n",
    "        raise ValueError(\"Shape mismatch: {} -> {}\".format(ndarray.shape,\n",
    "                                                           new_shape))\n",
    "    compression_pairs = [(d, c//d) for d, c in zip(new_shape,\n",
    "                                                   ndarray.shape)]\n",
    "    flattened = [l for p in compression_pairs for l in p]\n",
    "    ndarray = ndarray.reshape(flattened)\n",
    "    for i in range(len(new_shape)):\n",
    "        if operation.lower() == \"sum\":\n",
    "            ndarray = ndarray.sum(-1*(i+1))\n",
    "        elif operation.lower() in [\"mean\", \"average\", \"avg\"]:\n",
    "            ndarray = ndarray.mean(-1*(i+1))\n",
    "    return ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirbe_1deg = fits.open('SPI_DIRBE_Orig_Knoedl_240um.fits')\n",
    "data = dirbe_1deg[2].data\n",
    "\n",
    "# interpolate to (360, 720)\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "new_dims = []\n",
    "for original_length, new_length in zip(data.shape, (360, 720)):\n",
    "    new_dims.append(np.linspace(0, original_length-1, new_length))\n",
    "    \n",
    "coords = np.meshgrid(*new_dims, indexing='ij')\n",
    "B = map_coordinates(data, coords)\n",
    "B.shape\n",
    "\n",
    "dirbe_6deg_6deg = bin_ndarray(B, (30, 60), 'mean')\n",
    "dirbe_6deg_6deg = np.flip(dirbe_6deg_6deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set an initial guess for the background amplitude\n",
    "Feel free to play with this value, but here are suggestions informed by testing thus far:\n",
    "\n",
    "### If source+BG:\n",
    "We suggest setting \"fitted_bg\" to 0.9 or 0.99 when the loaded data/simulation (analysis1 object) contains both source and background. This is a rough estimate of the background contribution (90, 99%) to the entire data set.\n",
    "\n",
    "### If analyzing source only:\n",
    "When the analysis1 object does not contain background, we suggest setting this parameter to 1E-6, i.e. very close to zero background contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bg = np.array([0.9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richardson-Lucy algorithm\n",
    "\n",
    "## Individual steps are explained in code.\n",
    "The steps follow the algorithm as outlined in [Knödlseder et al. 1999](https://ui.adsabs.harvard.edu/abs/1999A%26A...345..813K/abstract). Refer to that paper for a mathematical description of the algorithm.\n",
    "\n",
    "The total memory used during these iterations is about 108 GB!! You might not be able to do much else with your machine while this is running. \n",
    "\n",
    "## Adjustable parameters\n",
    "There are three parameters at the beginning of this RL cell which we encourage you to adjust. In fact, it is often necessary to adjust these parameters depending on the data being studied.\n",
    "\n",
    "- map_init\\\n",
    "This is the flux value of the initial, isotropic map. Typically, a value of 0.01 works well. For stronger sources, you can try increasing it to 0.1 or 1.0. As an example, running the algorithm on a source-only (no BG) simulation of the Crab, Cen A, Cygnus X-1, and Vela works well with map_init = 0.01. However, when imaging these sources each simulated with 10X their true flux values, the algorithm fails at 0.01 and work when map_init = 1.0.\n",
    "\n",
    "- iterations\\\n",
    "This is the number of RL iterations. You can set this to a small value, say 50, as you get used to using the algorithm. In our testing, though, for fully converged images we usually let the algorithm run for 150 iterations. ***This can take anywhere from several hours (usually simulations without background) to overnight (simulations with background) to run.***\n",
    "\n",
    "- afl_scl\\\n",
    "This is a scaling factor for the delta map which we call the \"acceleration parameter.\" This allows the delta map to be afl_scl times stronger than the original RL algorithm suggests (c.f. Knoedlseder+1997).\\\n",
    "The default value here is 2000, though 1000 also works well. If you find that the algorithm returns \"Fit failed\" messages after running for awhile, for example, lowering this acceleration parameter to 1000 can help.\n",
    "\n",
    "Other parameters you can adjust:\n",
    "- mu_Abg, sigma_Abg\\\n",
    "There is a prior in the background fit defined by mu_Abg +/- sigma_Abg. By default, mu_Abg and sigma_Abg are set to fitted_bg and most testing has been done with this setting. You can try constraining the fit by decreasing sigma_Abg, for example, to sigma_Abg/2., sigma_Abg/10., which would enable to fit to vary by 50%, 10% of the initial guess.\n",
    "\n",
    "- delta_map_tot_old\\\n",
    "You can change the exponent of the denominator. By default, it is set to 0.25 to help avoid exposure edge effects. All testing has been done with this fourth root. However, you can try setting it to 0, 0.5, etc. to see what happens. You can also try smoothing delta_map_tot_old with a Gaussian filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might not use this depending on if you choose to smooth the delta map\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with these variables!\n",
    "#############################\n",
    "# initial map (isotropic flat, small value)\n",
    "map_init = IsoMap(L_ARR, B_ARR, 0.01)\n",
    "\n",
    "# number of RL iterations, Usually test with ~150 iterations. \n",
    "iterations = 150 \n",
    "\n",
    "# acceleration parameter\n",
    "afl_scl = 2000.\n",
    "#############################\n",
    "\n",
    "\n",
    "## Define background (to be sure it's the same as before)\n",
    "bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts\n",
    " \n",
    "    \n",
    "## Save intermediate iterations: initialise arrays to save images and other parameters\n",
    "# maps per iteration\n",
    "map_iterations = np.zeros((n_b, n_l, iterations))\n",
    "\n",
    "# likelihood of maps (vs. initial i.e. basically only background)\n",
    "map_likelihoods = np.zeros(iterations)\n",
    "\n",
    "# fit likelihoods, ie fit quality\n",
    "intermediate_lp = np.zeros(iterations)\n",
    "\n",
    "# acceleration parameters (lambda)\n",
    "acc_par = np.zeros(iterations)\n",
    "\n",
    "# fitted background parameters \n",
    "bg_pars = np.zeros((iterations,Ncuts))\n",
    "\n",
    "\n",
    "## Zeroth iteration: copy initial map to become the 'old map' (see below)\n",
    "map_old = map_init\n",
    "\n",
    "# cf. Knoedlseder+1997 what the values denominator etc are\n",
    "# this is the response R summed over the CDS and the time bins\n",
    "denominator = expo_map\n",
    "\n",
    "# zeroth iteration is then just the initial map\n",
    "map_iterations[:,:,0] = map_old#[]\n",
    "\n",
    "# convolve this map with the response\n",
    "expectation_init = 0\n",
    "print('Convolving with response (init expectation), iteration 0')\n",
    "for i in tqdm(range(n_b)):\n",
    "    for j in range(n_l):\n",
    "        expectation_init += sky_response_scaled[:,i,j,:]*map_init[i,j]\n",
    "\n",
    "# set old expectation (in data space bins) to new expectation (convolved image)\n",
    "expectation_old = expectation_init\n",
    "\n",
    "### now we have the expectation of the image. Need to go to the BG \n",
    "        \n",
    "###########################################################\n",
    "###########################################################\n",
    "## here run over the number of iterations #################\n",
    "###########################################################\n",
    "## the time for the convolutions is very large ############\n",
    "## this can be 10 minutes (!) per iteration ###############\n",
    "## this should be tested for a few iterations #############\n",
    "## and then run overnight or similar ######################\n",
    "###########################################################\n",
    "###########################################################\n",
    "for its in tqdm(range(1,iterations)):\n",
    "    \n",
    "    # setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "    map_old[bad_expo[0],bad_expo[1]] = 0\n",
    "    \n",
    "    # check for each pixel to be finite\n",
    "    map_old[np.where(np.isnan(map_old) == True)] = 0\n",
    "    \n",
    "    # make new background for the next iteration\n",
    "    bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts\n",
    "    \n",
    "    # temporary background model\n",
    "    tmp_model_bg = np.zeros((d2h,background1.bg_model_reduced[ebin].shape[1]))\n",
    "    \n",
    "    # there could be something different for the first iteration (here it isn't, same function call)\n",
    "    if its == 1:\n",
    "        for g in range(d2h):\n",
    "            tmp_model_bg[g,:] = background_model[g,:]*fitted_bg[idx_arr-1][g]\n",
    "    else:\n",
    "        for g in range(d2h):\n",
    "            tmp_model_bg[g,:] = background_model[g,:]*fitted_bg[idx_arr-1][g]\n",
    "            \n",
    "    # expectation (in data space) is the image (expectation_old) plus the background (tmp_model_bg)\n",
    "    expectation_tot_old = expectation_old + tmp_model_bg \n",
    "\n",
    "    # calculate likelihood of currect total expectation\n",
    "    map_likelihoods[its-1] = cashstat(dataset.ravel(),expectation_tot_old.ravel())\n",
    "    \n",
    "    # calculate numerator of RL algorithm\n",
    "    numerator = 0\n",
    "    print('Calculating Delta image, iteration '+str(its)+', numerator')\n",
    "    for i in tqdm(range(d2h)):\n",
    "        for j in range(dataset.shape[1]):\n",
    "            numerator += (dataset[i,j]/expectation_tot_old[i,j]-1)*sky_response_scaled[i,:,:,j]\n",
    "    \n",
    "    # calculate delta map (denominator scaled by fourth root to avoid exposure edge effects)\n",
    "    # You can try changing 0.25 to 0, 0.5, for example\n",
    "    delta_map_tot_old = (numerator/denominator)*map_old*(denominator)**0.25\n",
    "    \n",
    "    # Alternatively, you can also try to smooth it \n",
    "    #delta_map_tot_old = gaussian_filter(delta_map_tot_old, 0.5)\n",
    "    \n",
    "    #################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    # check again for finite values and zero our bad exposure regions\n",
    "    nan_idx = np.where(np.isnan(delta_map_tot_old) == 1)\n",
    "    delta_map_tot_old[nan_idx[0],nan_idx[1]] = 0\n",
    "    delta_map_tot_old[bad_expo[0],bad_expo[1]] = 0\n",
    "\n",
    "    # plot each iterations and its delta map \n",
    "    # (not required, but nice to see how the algorithm is doing)\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(map_old, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(delta_map_tot_old, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    # convolve delta image\n",
    "    print('Convolving Delta image, iteration '+str(its))\n",
    "    conv_delta_map_tot = 0\n",
    "    for i in tqdm(range(n_b)):\n",
    "        for j in range(n_l):\n",
    "            conv_delta_map_tot += sky_response_scaled[:,i,j,:]*delta_map_tot_old[i,j]\n",
    "    \n",
    "    # find maximum acceleration parameter to multiply delta image with\n",
    "    # so that the total image is still positive everywhere\n",
    "    print('Finding maximum acceleration parameter, iteration '+str(its))\n",
    "    try:\n",
    "        len_arr = []\n",
    "        for i in range(0,10000):\n",
    "            len_arr.append(len(np.where((map_old+delta_map_tot_old*i/afl_scl) < 0)[0]))\n",
    "        len_arr = np.array(len_arr)\n",
    "        afl = np.max(np.where(len_arr == 0)[0])\n",
    "        print('Maximum acceleration parameter found: ',afl/afl_scl)\n",
    "\n",
    "        \n",
    "        # fit delta map and current map to speed up RL algorithm\n",
    "        print('Fitting delta-map in addition to old map, iteration '+str(its))\n",
    "        # dictionary for data set and prior\n",
    "        # note that here the value for N should be your response CDS dimension\n",
    "        # should be last dimension of the scaled response thing (change to your value)\n",
    "        data_multimap = dict(N = dataset.shape[1],\n",
    "                     Nh = d2h,\n",
    "                     Ncuts = Ncuts,\n",
    "                     Nsky = 2,\n",
    "                     acceleration_factor_limit=afl*0.95,\n",
    "                     bg_cuts = bg_cuts,\n",
    "                     bg_idx_arr = idx_arr,\n",
    "                     y = dataset.ravel().astype(int),\n",
    "                     bg_model = tmp_model_bg,\n",
    "                     conv_sky = np.concatenate([[expectation_old],[conv_delta_map_tot/afl_scl]]),\n",
    "                     mu_flux = np.array([1,afl/2]),\n",
    "                     sigma_flux = np.array([1e-2,afl]),\n",
    "                     mu_Abg = fitted_bg,    # can play with this\n",
    "                     sigma_Abg = fitted_bg) # can play with this\n",
    "\n",
    "        # fit;\n",
    "        # initial values for fit (somewhat sensitive here with COSI data)\n",
    "        init = {}\n",
    "        init['flux'] = np.array([1.,afl/2.])\n",
    "        init['Abg'] = np.repeat(fitted_bg, Ncuts) # can play with this\n",
    "        \n",
    "        # fit: might take some time but it shouldn't be more than a minute\n",
    "        op2D = model_multimap.optimizing(data=data_multimap,init=init,as_vector=False,verbose=True,\n",
    "                                                tol_rel_grad=1e3,tol_obj=1e-20)\n",
    "\n",
    "        # save values\n",
    "        print('Saving new map, and fitted parameters, iteration '+str(its))\n",
    "        intermediate_lp[its-1] = op2D['value']\n",
    "        acc_par[its-1] = op2D['par']['flux'][1]\n",
    "        bg_pars[its-1,:] = op2D['par']['Abg']\n",
    "  \n",
    "        # make new map as old map plus scaled delta map\n",
    "        map_new = map_old+op2D['par']['flux'][1]*delta_map_tot_old/afl_scl\n",
    "    \n",
    "        # same with expectation (data space)\n",
    "        expectation_new = expectation_old + op2D['par']['flux'][1]*conv_delta_map_tot/afl_scl\n",
    "        \n",
    "\n",
    "    except:\n",
    "        # if the fit failed...\n",
    "        # this shouldn't happen too often (or at all)\n",
    "        print('############## Fit failed! proceeding without acceleration ##############')\n",
    "        map_new = map_old + delta_map_tot_old\n",
    "        expectation_new = expectation_old + conv_delta_map_tot\n",
    "    \n",
    "    # check finite values again\n",
    "    if its == 1:\n",
    "        bad_index_init = np.where(np.isnan(map_new) == True)\n",
    "    \n",
    "    # also here\n",
    "    map_new[bad_expo[0],bad_expo[1]] = 0\n",
    "    map_new[np.where(np.isnan(map_new) == True)] = 0\n",
    "    map_iterations[:,:,its] = map_new\n",
    "\n",
    "    # swap maps\n",
    "    map_old = map_new\n",
    "    \n",
    "    # and expectations\n",
    "    expectation_old = expectation_new\n",
    "    \n",
    "    # and repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the fitted background parameter and the map flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(its), [i[0] for i in bg_pars[:its]], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('BG params]')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "map_fluxes = np.zeros(its)\n",
    "for i in range(its):\n",
    "    map_fluxes[i] = np.sum(map_iterations[:,:,i]*domega)\n",
    "    \n",
    "plt.plot(map_fluxes[:its],'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Flux')# [ph/keV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the algorithm converge? Look at the likelihoods.\n",
    "intermediate_lp: Fit likelihoods, i.e. fit quality\n",
    "\n",
    "map_likelihoods: likelihood of maps (vs. initial i.e. basically only background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(its+1)[:-1], intermediate_lp[:its+1][:-1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (intermediate_lp)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(its+1)[:-1], map_likelihoods[:its+1][:-1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (map_likelihoods)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the image!\n",
    "You can loop over all iterations to make a GIF or just show one iteration (usually the final iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import Video\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "from scipy.ndimage import gaussian_filter as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image to plot\n",
    "idx = its - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a color map like viridis (matplotlib default), nipy_spectral, twilight_shifted, etc. Not jet.\n",
    "cmap = plt.get_cmap('viridis') \n",
    "\n",
    "# Bad exposures will be gray\n",
    "cmap.set_bad('lightgray')\n",
    "\n",
    "\n",
    "##################\n",
    "# Select here which pixels should be gray\n",
    "map_iterations_nan = np.copy(map_iterations)\n",
    "\n",
    "# Select also non-zero exposures here to be gray (avoiding the edge effects)\n",
    "# You can play with this. Most success in testing with 1e4, 1e3\n",
    "bad_expo = np.where(expo_map/domega <= 1e3) \n",
    "\n",
    "for i in range(iterations):\n",
    "    map_iterations_nan[bad_expo[0], bad_expo[1], i] = np.nan\n",
    "#################    \n",
    "\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10.24,7.68), subplot_kw={'projection':'aitoff'}, nrows=1, ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120,-60,0,60,120])*deg2rad)\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "plt.xlabel('Gal. Lon. [deg]')\n",
    "plt.ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "\n",
    "# Optionally plot contours from the DIRBE 240um image\n",
    "levels = [np.max(dirbe_6deg_6deg)*0.05, np.max(dirbe_6deg_6deg)*0.1,\n",
    "          np.max(dirbe_6deg_6deg)*0.5, np.max(dirbe_6deg_6deg)*0.8]\n",
    "plt.contour(L_ARR*deg2rad, B_ARR*deg2rad, dirbe_6deg_6deg, levels=levels, colors='white', alpha=1)\n",
    "\n",
    "\n",
    "# \"ims\" is a list of lists, each row is a list of artists to draw in the\n",
    "# current frame; here we are just animating one artist, the image, in\n",
    "# each frame\n",
    "ims = []\n",
    "\n",
    "\n",
    "# If you want to make a GIF of all iterations:\n",
    "#for i in range(iterations):\n",
    "\n",
    "# If you only want to plot one image:\n",
    "for i in [idx]:\n",
    "\n",
    "    ttl = plt.text(0.5, 1.01, f'RL iteration {i}', horizontalalignment='center', \n",
    "                   verticalalignment='bottom', transform=ax.transAxes)\n",
    "    \n",
    "    # Either gray-out bad exposure (map_iterations_nan) or don't mask (map_iterations)\n",
    "    # Masking out bad exposure \n",
    "    #image = map_iterations_nan[:, :, i]\n",
    "    image = map_iterations[:, :, i]\n",
    "\n",
    "    \n",
    "    img = ax.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,\n",
    "                        \n",
    "                        # Can shift the image along longitude. Here, no shift.\n",
    "                        np.roll(image, axis=1, shift=0),\n",
    "            \n",
    "                        # Optionally smooth with gaussian filter\n",
    "                        #smooth(np.roll(image, axis=1, shift=0), 0.75/pixel_size),\n",
    "                        \n",
    "                        cmap=plt.cm.viridis,\n",
    "                        \n",
    "                        # Optionally set the color scale. Default: linear\n",
    "                        #norm=colors.PowerNorm(0.33)\n",
    "                       )\n",
    "    ax.grid()\n",
    "    \n",
    "    ims.append([img, ttl])\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "#cbar.ax.set_xlabel(r'Flux [10$^{-2}$ ph cm$^{-2}$ s$^{-1}$]')\n",
    "    \n",
    "\n",
    "# Can save a sole image as a PDF \n",
    "#plt.savefig(data_dir + f'images/26Al_RL_image_iteration{idx}.pdf', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "# # Can save all iterations as a GIF\n",
    "# ani = animation.ArtistAnimation(fig, ims, interval=200, blit=True, repeat_delay=0)\n",
    "# ani.save(f'/home/jacqueline/26Al_RL_image_{idx}iterations.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we see?\n",
    "\n",
    "As expected, we observe extended $^{26}$Al emission along the Galactic Plane. There is concentrated emission in the Inner Galaxy. The RL algorithm therefore behaves as expected, placing photons simulated along the DIRBE 240$\\mu$m template map within the contours of this chosen tracer of $^{26}$Al emission.\n",
    "\n",
    "Given that only $\\sim$100 $^{26}$Al photons were detected during the COSI-balloon flight ($3.7\\sigma$ significance, [Beechert et al. 2022](https://iopscience.iop.org/article/10.3847/1538-4357/ac56dc/meta)), imaging the emission at its true flux instead of 10x strength would likely result in only imaging artifacts. Consider the following calculation. For $n$ spatial bins each with measurement significance $n_i$, the total significance of a measurement is \n",
    "\n",
    "$s = (\\sum_{i = 1}^{n} s_i^2)^{1/2}$.\n",
    "\n",
    "Even requiring only a weak $2\\sigma$ measurement in each bin, the maximum $n$ number of bins for a total $3.7\\sigma$ measurement is approximately 3.4. In other words, a $3.7\\sigma$ significant measurement distributed across the broad, diffuse $^{26}$Al emission which spans the Galactic Plane would result in few spatial bins with discernable significance. \n",
    "\n",
    "Below, we fit the simulated 10x flux image with a 2-D Gaussian as a demonstration of efforts to characterize the morphology of Galactic $^{26}$Al. \n",
    "\n",
    "Future data challenges will image $^{26}$Al as seen with the COSI satellite. Increased observation time, increased effective area, finer angular resolution, and observations at high Galactic latitudes (extending above and below the Galactic Plane) have great potential to advance understanding of this radioisotope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a 2D Gaussian to the emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_2d(xtuple, A, x0, y0, sigma_x, sigma_y, theta):\n",
    "    # theta: rotate the blob by positive, counterclockwise angle theta\n",
    "    (x, y) = xtuple\n",
    "    x0 = float(x0)\n",
    "    y0 = float(y0)\n",
    "    a = np.cos(theta)**2/(2*sigma_x**2) + np.sin(theta)**2/(2*sigma_y**2)\n",
    "    b = np.sin(2*theta)/(4*sigma_x**2) - np.sin(2*theta)/(4*sigma_y**2)\n",
    "    c = np.sin(theta)**2/(2*sigma_x**2) + np.cos(theta)**2/(2*sigma_y**2)\n",
    "    tot = A*np.exp( -( a*(x-x0)**2 + 2*b*(x-x0)*(y-y0) + c*(y-y0)**2 ) )\n",
    "    return tot.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "initial_guess = (2, 0, 0, 10, 10, 4)\n",
    "x = (L_ARRg*deg2rad)[:-1, :-1]\n",
    "y = (B_ARRg*deg2rad)[:-1, :-1]\n",
    "z = map_iterations_nan[:, :, idx]\n",
    "nan = np.isnan(z)\n",
    "z[nan] = 0.\n",
    "popt, pcov = opt.curve_fit(gauss_2d, (x, y), z.ravel(), p0=initial_guess)\n",
    "\n",
    "im_fitted = gauss_2d((x, y), *popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10.24, 7.68), subplot_kw={'projection':'aitoff'}, nrows=1, ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120, -60, 0, 60, 120])*deg2rad)\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "ax.set_xlabel('Gal. Lon. [deg]')\n",
    "ax.set_ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "# Plot original image\n",
    "ax.pcolormesh(L_ARRg*deg2rad, B_ARRg*deg2rad, z.reshape(len(x), len(x[0])), cmap=plt.cm.viridis)\n",
    "\n",
    "# Plot contours\n",
    "num_contours = 2\n",
    "levels = [np.max(im_fitted)*0.05, np.max(im_fitted)*0.1,\n",
    "          np.max(im_fitted)*0.5, np.max(im_fitted)*0.8]\n",
    "\n",
    "plt.contour(L_ARR*deg2rad, B_ARR*deg2rad, im_fitted.reshape(len(x), len(x[0])), levels=levels, colors='white')\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "#cbar.ax.set_xlabel(r'Flux [10$^{-2}$ ph cm$^{-2}$ s$^{-1}$]')\n",
    "    \n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A:', popt[0])\n",
    "print('x0 [deg]:', popt[1]*180/np.pi)\n",
    "print('y0 [deg]:', popt[2]*180/np.pi)\n",
    "print('sigma_x [deg]:', popt[3]*180/np.pi, '--> FWHM_x [deg]:', 2*np.sqrt(2*np.log(2))*popt[3]*180/np.pi)\n",
    "print('sigma_y [deg]:', popt[4]*180/np.pi, '--> FWHM_y [deg]:', 2*np.sqrt(2*np.log(2))*popt[4]*180/np.pi)\n",
    "print('theta [deg]:', popt[5]*180/np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
