{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to 511 keV imaging with cosipy-classic\n",
    "In this notebook, we'll use a Richardson-Lucy deconvolution algorithm to image 511 keV emission from the center of the Milky Way Galaxy. This analysis requires significant computer memory (>50 GB), so you may want to use a more resource-intensive computer for this work. Please refer to the README for additional information on each step of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "We will need to import the cosipy-classic functions from COSIpy_dc1.py, response_dc1, and COSIpy_tools_dc1, as well as some other standard Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from COSIpy_dc1 import *\n",
    "import response_dc1\n",
    "from COSIpy_tools_dc1 import *\n",
    "\n",
    "import pickle\n",
    "import pystan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define file names\n",
    "This file contains the 10X flux 511 keV simulation and Ling BG. \n",
    "\n",
    "You can optionally image only 511 keV (without background) by changing this file to the 511 keV-only simulation. You will have to adjust the RL algorithm parameters later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data_products' # directory containing data & response files\n",
    "filename = 'GC511_10xFlux_and_Ling.inc1.id1.extracted.tra.gz'# 511 keV with Ling BG\n",
    "response_filename = data_dir + '/511keV_imaging_response.npz' # detector response\n",
    "background_filename = data_dir + '/Scaled_Ling_BG_1x.npz' # background response\n",
    "background_mode = 'from file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read simulation and define analysis object\n",
    "Read in the data set and create the main cosipy-classic “analysis1\" object, which provides various functionalities to study the specified file. This cell usually takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1 = COSIpy(data_dir, filename)\n",
    "analysis1.read_COSI_DataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin the data\n",
    "Calling \"get_binned_data()\" may take several minutes, depending on the size of the dataset and the number of bins. Keep an eye on memory here: if your time bins are very small, for example, this could be an expensive operation.\n",
    "\n",
    "As currently written, \"get_binned_data()\" uses about **4 GB memory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin sizes\n",
    "Delta_T = 1800 # time bin size in seconds\n",
    "energy_bin_edges = np.array([501, 521]) # as defined in the response\n",
    "pixel_size = 6. # as defined in the response\n",
    "\n",
    "analysis1.dataset.time_binning_tags(time_bin_size=Delta_T)\n",
    "analysis1.dataset.init_binning(energy_bin_edges=energy_bin_edges, pixel_size=pixel_size) # initiate the binning\n",
    "analysis1.dataset.get_binned_data() # bin data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the shape of the binned data.\n",
    "The binned data are contained in \"analysis1.dataset.binned_data.\" This is a 4-dimensional object representing the 5 dimensions of the Compton data space: (time, energy, $\\phi$, FISBEL).\n",
    "\n",
    "The number of bins in each dimension are shown by calling \"shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time, energy, phi, fisbel\")\n",
    "print(analysis1.dataset.binned_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can print the number of time bins, the width of each time bin, and the total time\n",
    "print(analysis1.dataset.times.n_time_bins)\n",
    "print(analysis1.dataset.times.times_wid)\n",
    "print(analysis1.dataset.times.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot raw spectrum & light curve\n",
    "For a single energy bin, the spectrum is necessarily a top hat in the sole non-zero bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1.dataset.plot_lightcurve()\n",
    "\n",
    "analysis1.dataset.plot_raw_spectrum()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pointing object with the cosipy pointing class.\n",
    "This may also take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of poitings (balloon stability + Earth rotation)\n",
    "pointing1 = Pointing(dataset=analysis1.dataset,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the BG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ling BG simulation to model atmospheric background\n",
    "background1 = BG(dataset=analysis1.dataset,mode=background_mode,filename=background_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Response Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 511 keV response\n",
    "rsp = response_dc1.SkyResponse(filename=response_filename,pixel_size=pixel_size) # read in detector response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the shape of the data space\n",
    "The shape of the response spans (Galactic latitude $b$, Galactic longitude $\\ell$, Compton scattering angle $\\phi$,  FISBEL, energy). There is 1 energy bin for the 511 keV response (\"analysis1.dataset.energies.n_energy_bins\"). This is why there is no fifth dimension for the energy printed below. The shape of the data and background objects span (time, Compton scattering angle, FISBEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp.rsp.response_grid_normed_efinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(analysis1.dataset.binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(background1.bg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a grid on the sky to make images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenient variable for deg --> radian conversion\n",
    "deg2rad = np.pi/180.\n",
    "\n",
    "# We define our sky-grid on a regular (pixel_size x pixel_size) grid for testing (later finer grid)\n",
    "binsize = pixel_size\n",
    "\n",
    "# Galactic coordiantes: l and b pixel edges\n",
    "l_arrg = np.linspace(-180, 180, int(360/binsize)+1)\n",
    "b_arrg = np.linspace(-90, 90, int(180/binsize)+1)\n",
    "\n",
    "# Number of pixels in l and b\n",
    "n_l = int(360/binsize)\n",
    "n_b = int(180/binsize)\n",
    "\n",
    "# Making a grid\n",
    "L_ARRg, B_ARRg = np.meshgrid(l_arrg, b_arrg)\n",
    "\n",
    "# Choosing the centre points as representative\n",
    "l_arr = l_arrg[0:-1] + binsize/2\n",
    "b_arr = b_arrg[0:-1] + binsize/2\n",
    "L_ARR, B_ARR = np.meshgrid(l_arr, b_arr)\n",
    "\n",
    "# Define solid angle for each pixel for normalisations later\n",
    "domega = (binsize*deg2rad)*(np.sin(np.deg2rad(B_ARR + binsize/2)) - np.sin(np.deg2rad(B_ARR - binsize/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sky grid to zenith/azimuth pairs for all pointings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the zeniths and azimuths on that grid for all times\n",
    "zensgrid,azisgrid = zenaziGrid(pointing1.ypoins[:,0], pointing1.ypoins[:,1],\n",
    "                               pointing1.xpoins[:,0], pointing1.xpoins[:,1],\n",
    "                               pointing1.zpoins[:,0], pointing1.zpoins[:,1],\n",
    "                               L_ARR.ravel(), B_ARR.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for next routines ... \n",
    "zensgrid = zensgrid.reshape(n_b, n_l, len(pointing1.xpoins))\n",
    "azisgrid = azisgrid.reshape(n_b, n_l, len(pointing1.xpoins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get observation indices for non-zero bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an energy bin to analyze\n",
    "ebin = 0 # We only have one energy bin (501-521 keV), so the index is necessarily 0.\n",
    "nonzero_idx = background1.calc_this[ebin]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the response dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_CDS = rsp.rsp.response_grid_normed_efinal.reshape(\n",
    "    n_b,\n",
    "    n_l,\n",
    "    analysis1.dataset.phis.n_phi_bins*\\\n",
    "    analysis1.dataset.fisbels.n_fisbel_bins, analysis1.dataset.energies.n_energy_bins)[:, :, nonzero_idx, ebin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced response dimensions:\n",
    "# lat x lon x CDS\n",
    "sky_response_CDS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the response of an image for arbitrary time binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_response_from_pixelhit_general(Response,zenith,azimuth,dt,n_hours,binsize=6,cut=60,altitude_correction=False,al=None):\n",
    "    \"\"\"\n",
    "    Get Compton response from hit pixel for each zenith/azimuth vector(!) input.\n",
    "    Binsize determines regular(!!!) sky coordinate grid in degrees.\n",
    "\n",
    "    :param: zenith        Zenith positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :param: azimuth       Azimuth positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :option: binsize      Default 5 deg (matching the sky dimension of the response). If set\n",
    "                          differently, make sure it matches the sky dimension as otherwise,\n",
    "                          false results may be returned\n",
    "    :option: cut          Threshold to cut the response calculation after a certain zenith angle.\n",
    "                          Default 60\n",
    "    :param: n_hours       Number of hours in cdxervation\n",
    "    :option: altitude_correction Default False: use interpolated transmission probability, normalised to 33 km and 500 keV,\n",
    "                          to modify number of expected photons as a function of altitude and zenith angle of cdxervation\n",
    "    :option: al           Altitude values according to dt from construct_pointings(); used of altitude_correction is set to True\n",
    "    \"\"\"\n",
    "\n",
    "    # assuming useful input:\n",
    "    # azimuthal angle is periodic in the range [0,360[\n",
    "    # zenith ranges from [0,180[\n",
    "\n",
    "    # check which pixel (index) was hit on regular grid\n",
    "    hit_pixel_zi = np.floor(zenith/binsize)\n",
    "    hit_pixel_ai = np.floor(azimuth/binsize)\n",
    "\n",
    "    # and which pixel centre\n",
    "    hit_pixel_z = (hit_pixel_zi+0.5)*binsize\n",
    "    hit_pixel_a = (hit_pixel_ai+0.5)*binsize\n",
    "\n",
    "    # check which zeniths are beyond threshold\n",
    "    bad_idx = np.where(hit_pixel_z > cut)\n",
    "\n",
    "    # set hit pixels to output array\n",
    "    za_idx = np.array([hit_pixel_zi,hit_pixel_ai]).astype(int)\n",
    "\n",
    "    nz = zenith.shape[2]\n",
    "\n",
    "    n_lon = int(360/binsize)\n",
    "    n_lat = int(180/binsize)\n",
    "    \n",
    "    l_arrg = np.linspace(-180,180,int(360/binsize)+1)\n",
    "    b_arrg = np.linspace(-90,90,int(180/binsize)+1)\n",
    "    L_ARRg, B_ARRg = np.meshgrid(l_arrg,b_arrg)\n",
    "    l_arr = l_arrg[0:-1]+binsize/2\n",
    "    b_arr = b_arrg[0:-1]+binsize/2\n",
    "    L_ARR, B_ARR = np.meshgrid(l_arr,b_arr)\n",
    "\n",
    "    # take care of regular grid by applying weighting with latitude\n",
    "    weights = ((binsize*np.pi/180)*(np.sin(np.deg2rad(B_ARR+binsize/2)) - np.sin(np.deg2rad(B_ARR-binsize/2)))).repeat(nz).reshape(n_lat,n_lon,nz)\n",
    "    weights[bad_idx] = 0\n",
    "\n",
    "    \n",
    "    # check for negative weights and indices and remove\n",
    "    weights[za_idx[0,:] < 0] = 0.\n",
    "    weights[za_idx[1,:] < 0] = 0.\n",
    "    za_idx[0,za_idx[0,:] < 0] = 0.\n",
    "    za_idx[1,za_idx[1,:] < 0] = 0.\n",
    "        \n",
    "    \n",
    "    if altitude_correction == True:\n",
    "        altitude_response = return_altitude_response()\n",
    "    else:\n",
    "        altitude_response = one_func\n",
    "\n",
    "    # get responses at pixels    \n",
    "    image_response = np.zeros((n_hours,n_lat,n_lon,Response.shape[2]))\n",
    "\n",
    "    for c in tqdm(range(n_hours)):\n",
    "        cdx = np.where((pointing1.cdtpoins > analysis1.dataset.times.times_min[analysis1.dataset.times.n_ph_dx[c]]) &\n",
    "                       (pointing1.cdtpoins <= analysis1.dataset.times.times_max[analysis1.dataset.times.n_ph_dx[c]]))[0]\n",
    "    \n",
    "        # this calculation is basically a look-up of the response entries. In general, weighting (integration) with the true shape can be introduced, however with a lot more computation time (Simpson's rule in 2D ...)\n",
    "        image_response[c,:,:,:] += np.sum(Response[za_idx[0,:,:,cdx],za_idx[1,:,:,cdx],:]*np.einsum('klij->iklj', weights[:,:,cdx,None])*dt[cdx,None,None,None],axis=0)#*altitude_weights[:,:,None]\n",
    "        \n",
    "    return image_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the general response for the current data set\n",
    "This has to be done only once (for the data set).\n",
    "\n",
    "Takes ~20 minutes to run and ~60 GB memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 90 # This mirrors the earth horizon cut in the response\n",
    "sky_response_scaled = get_image_response_from_pixelhit_general(\n",
    "    Response=sky_response_CDS,\n",
    "    zenith=zensgrid,\n",
    "    azimuth=azisgrid,\n",
    "    dt=pointing1.dtpoins,\n",
    "    n_hours=analysis1.dataset.times.n_ph,\n",
    "    binsize=pixel_size,\n",
    "    cut=cut,\n",
    "    altitude_correction=False,\n",
    "    al=np.ones(len(pointing1.dtpoins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-set-specific response dimensions\n",
    "# times x lat x lon x CDS\n",
    "sky_response_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure map\n",
    "i.e. the response weighted by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expo_map = np.zeros((n_b, n_l))\n",
    "\n",
    "for i in tqdm(range(sky_response_scaled.shape[0])):\n",
    "    expo_map += np.sum(sky_response_scaled[i,:,:,:], axis=2)\n",
    "    \n",
    "print(f'expo_map shape: {expo_map.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the exposure map weighted with the pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(projection='aitoff')\n",
    "p = plt.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,np.roll(expo_map/domega,axis=1,shift=0))\n",
    "plt.contour(L_ARR*deg2rad,B_ARR*deg2rad,np.roll(expo_map/domega,axis=1,shift=0),colors='black')\n",
    "plt.colorbar(p, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define regions of the sky that we actually cannot see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we select everything, i.e. we have no bad exposure\n",
    "\n",
    "bad_expo = np.where(expo_map/domega <= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for a starting map for the RL deconvolution. We choose an isotropic map, i.e. all pixels on the sky are initialized with the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsoMap(ll,bb,A0,binsize=pixel_size):\n",
    "    shape = np.ones(ll.shape)\n",
    "    norm = np.sum(shape*(binsize*np.pi/180)*(np.sin(np.deg2rad(bb+binsize/2)) - np.sin(np.deg2rad(bb-binsize/2))))\n",
    "    val = A0*shape/norm\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of time bins (should be the first dimension of the response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2h = analysis1.dataset.times.n_time_bins\n",
    "d2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only one energy bin (as above) for data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ebin: ',ebin)\n",
    "dataset = analysis1.dataset.binned_data[:,ebin,:,:].reshape(d2h,\n",
    "                                                            analysis1.dataset.phis.n_phi_bins*analysis1.dataset.fisbels.n_fisbel_bins)[:,nonzero_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same for background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = background1.bg_model_reduced[ebin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency of data and background\n",
    "They must have the same dimensions. If not, the algorithm won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape,background_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define background model cuts, indices, and resulting number of cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the modified RL algorithm implemented here, we need to load a stan model that fits background plus two images (the current image plus a delta image given by the RL formalism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"test1\")\n",
    "try:\n",
    "    #read stanmodel.pkl (if already compiled)\n",
    "    print(\"test2\")\n",
    "    model_multimap = pickle.load(open('stanmodel.pkl', 'rb'))\n",
    "\n",
    "except:\n",
    "    print('Model not yet compiled, doing that now (might take a while).')\n",
    "    \n",
    "    # compile model (if not yet compiled):\n",
    "    model_multimap = pystan.StanModel('stanmodel.stan')\n",
    "\n",
    "    with open('stanmodel.pkl', 'wb') as f:\n",
    "        pickle.dump(model_multimap, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"I think it works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('stanmodel.pkl', 'wb') as f:\n",
    "    pickle.dump(model_multimap, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set an initial guess for the background amplitude\n",
    "Feel free to play with this value, but here are suggestions informed by testing thus far:\n",
    "\n",
    "### If source+BG:\n",
    "We suggest setting \"fitted_bg\" to 0.9 or 0.99 when the loaded data/simulation (analysis1 object) contains both source and background. This is a rough estimate of the background contribution (90, 99%) to the entire data set.\n",
    "\n",
    "### If analyzing source only:\n",
    "When the analysis1 object does not contain background, we suggest setting this parameter to 1E-6, i.e. very close to zero background contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bg = np.array([0.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richardson-Lucy algorithm\n",
    "\n",
    "## Individual steps are explained in code.\n",
    "The steps follow the algorithm as outlined in [Knödlseder et al. 1999](https://ui.adsabs.harvard.edu/abs/1999A%26A...345..813K/abstract). Refer to that paper for a mathematical description of the algorithm.\n",
    "\n",
    "The total memory used during these iterations is about 74 GB!! You might not be able to do much else with your machine while this is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might not use this depending on if you choose to smooth the delta map\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with these variables!\n",
    "#############################\n",
    "# initial map (isotropic flat, small value)\n",
    "map_init = IsoMap(L_ARR, B_ARR, 0.01)\n",
    "\n",
    "# number of RL iterations, Usually test with ~50 iterations, and we can get fully conveerged images with ~150 iterations. \n",
    "iterations = 150 \n",
    "\n",
    "# acceleration parameter\n",
    "afl_scl = 1000.\n",
    "#############################\n",
    "\n",
    "\n",
    "## Define background (to be sure it's the same as before)\n",
    "bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts\n",
    " \n",
    "    \n",
    "## Save intermediate iterations: initialise arrays to save images and other parameters\n",
    "# maps per iteration\n",
    "map_iterations = np.zeros((n_b, n_l, iterations))\n",
    "\n",
    "# likelihood of maps (vs. initial i.e. basically only background)\n",
    "map_likelihoods = np.zeros(iterations)\n",
    "\n",
    "# fit likelihoods, ie fit quality\n",
    "intermediate_lp = np.zeros(iterations)\n",
    "\n",
    "# acceleration parameters (lambda)\n",
    "acc_par = np.zeros(iterations)\n",
    "\n",
    "# fitted background parameters \n",
    "bg_pars = np.zeros((iterations,Ncuts))\n",
    "\n",
    "\n",
    "## Zeroth iteration: copy initial map to become the 'old map' (see below)\n",
    "map_old = map_init\n",
    "\n",
    "# cf. Knoedlseder+1997 what the values denominator etc are\n",
    "# this is the response R summed over the CDS and the time bins\n",
    "denominator = expo_map\n",
    "\n",
    "# zeroth iteration is then just the initial map\n",
    "map_iterations[:,:,0] = map_old#[]\n",
    "\n",
    "# convolve this map with the response\n",
    "expectation_init = 0\n",
    "print('Convolving with response (init expectation), iteration 0')\n",
    "for i in tqdm(range(n_b)):\n",
    "    for j in range(n_l):\n",
    "        expectation_init += sky_response_scaled[:,i,j,:]*map_init[i,j]\n",
    "\n",
    "# set old expectation (in data space bins) to new expectation (convolved image)\n",
    "expectation_old = expectation_init\n",
    "\n",
    "### now we have the expectation of the image. Need to go to the BG \n",
    "        \n",
    "###########################################################\n",
    "###########################################################\n",
    "## here run over the number of iterations #################\n",
    "###########################################################\n",
    "## the time for the convolutions is very large ############\n",
    "## this can be 10 minutes (!) per iteration ###############\n",
    "## this should be tested for a few iterations #############\n",
    "## and then run overnight or similar ######################\n",
    "###########################################################\n",
    "###########################################################\n",
    "for its in tqdm(range(1,iterations)):\n",
    "    \n",
    "    # setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "    map_old[bad_expo[0],bad_expo[1]] = 0\n",
    "    \n",
    "    # check for each pixel to be finite\n",
    "    map_old[np.where(np.isnan(map_old) == True)] = 0\n",
    "    \n",
    "    # make new background for the next iteration\n",
    "    bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts\n",
    "    \n",
    "    # temporary background model\n",
    "    tmp_model_bg = np.zeros((d2h,background1.bg_model_reduced[ebin].shape[1]))\n",
    "    \n",
    "    # there could be something different for the first iteration (here it isn't, same function call)\n",
    "    if its == 1:\n",
    "        for g in range(d2h):\n",
    "            tmp_model_bg[g,:] = background_model[g,:]*fitted_bg[idx_arr-1][g]\n",
    "    else:\n",
    "        for g in range(d2h):\n",
    "            tmp_model_bg[g,:] = background_model[g,:]*fitted_bg[idx_arr-1][g]\n",
    "            \n",
    "    # expectation (in data space) is the image (expectation_old) plus the background (tmp_model_bg)\n",
    "    expectation_tot_old = expectation_old + tmp_model_bg \n",
    "\n",
    "    # calculate likelihood of currect total expectation\n",
    "    map_likelihoods[its-1] = cashstat(dataset.ravel(),expectation_tot_old.ravel())\n",
    "    \n",
    "    # calculate numerator of RL algorithm\n",
    "    numerator = 0\n",
    "    print('Calculating Delta image, iteration '+str(its)+', numerator')\n",
    "    for i in tqdm(range(d2h)):\n",
    "        for j in range(dataset.shape[1]):\n",
    "            numerator += (dataset[i,j]/expectation_tot_old[i,j]-1)*sky_response_scaled[i,:,:,j]\n",
    "    \n",
    "    # calculate delta map (denominator scaled by fourth root to avoid exposure edge effects)\n",
    "    # You can try changing 0.25 to 0, 0.5, for example\n",
    "    delta_map_tot_old = (numerator/denominator)*map_old*(denominator)**0.25\n",
    "    \n",
    "    # Alternatively, you can also try to smooth it \n",
    "    #delta_map_tot_old = gaussian_filter(delta_map_tot_old, 0.5)\n",
    "    \n",
    "    #################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    # check again for finite values and zero our bad exposure regions\n",
    "    nan_idx = np.where(np.isnan(delta_map_tot_old) == 1)\n",
    "    delta_map_tot_old[nan_idx[0],nan_idx[1]] = 0\n",
    "    delta_map_tot_old[bad_expo[0],bad_expo[1]] = 0\n",
    "\n",
    "    # plot each iterations and its delta map \n",
    "    # (not required, but nice to see how the algorithm is doing)\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(map_old, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(delta_map_tot_old, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    # convolve delta image\n",
    "    print('Convolving Delta image, iteration '+str(its))\n",
    "    conv_delta_map_tot = 0\n",
    "    for i in tqdm(range(n_b)):\n",
    "        for j in range(n_l):\n",
    "            conv_delta_map_tot += sky_response_scaled[:,i,j,:]*delta_map_tot_old[i,j]\n",
    "    \n",
    "    # find maximum acceleration parameter to multiply delta image with\n",
    "    # so that the total image is still positive everywhere\n",
    "    print('Finding maximum acceleration parameter, iteration '+str(its))\n",
    "    try:\n",
    "        len_arr = []\n",
    "        for i in range(0,10000):\n",
    "            len_arr.append(len(np.where((map_old+delta_map_tot_old*i/afl_scl) < 0)[0]))\n",
    "        len_arr = np.array(len_arr)\n",
    "        afl = np.max(np.where(len_arr == 0)[0])\n",
    "        print('Maximum acceleration parameter found: ',afl/afl_scl)\n",
    "\n",
    "        \n",
    "        # fit delta map and current map to speed up RL algorithm\n",
    "        print('Fitting delta-map in addition to old map, iteration '+str(its))\n",
    "        # dictionary for data set and prior\n",
    "        # note that here the value for N should be your response CDS dimension\n",
    "        # should be last dimension of the scaled response thing (change to your value)\n",
    "        data_multimap = dict(N = dataset.shape[1],\n",
    "                     Nh = d2h,\n",
    "                     Ncuts = Ncuts,\n",
    "                     Nsky = 2,\n",
    "                     acceleration_factor_limit=afl*0.95,\n",
    "                     bg_cuts = bg_cuts,\n",
    "                     bg_idx_arr = idx_arr,\n",
    "                     y = dataset.ravel().astype(int),\n",
    "                     bg_model = tmp_model_bg,\n",
    "                     conv_sky = np.concatenate([[expectation_old],[conv_delta_map_tot/afl_scl]]),\n",
    "                     mu_flux = np.array([1,afl/2]),\n",
    "                     sigma_flux = np.array([1e-2,afl]),\n",
    "                     mu_Abg = fitted_bg,    # can play with this\n",
    "                     sigma_Abg = fitted_bg) # can play with this\n",
    "\n",
    "        # fit;\n",
    "        # initial values for fit (somewhat sensitive here with COSI data)\n",
    "        init = {}\n",
    "        init['flux'] = np.array([1.,afl/2.])\n",
    "        init['Abg'] = np.repeat(fitted_bg, Ncuts) # can play with this\n",
    "        \n",
    "        # fit: might take some time but it shouldn't be more than a minute\n",
    "        op2D = model_multimap.optimizing(data=data_multimap,init=init,as_vector=False,verbose=True,\n",
    "                                                tol_rel_grad=1e3,tol_obj=1e-20)\n",
    "\n",
    "        # save values\n",
    "        print('Saving new map, and fitted parameters, iteration '+str(its))\n",
    "        intermediate_lp[its-1] = op2D['value']\n",
    "        acc_par[its-1] = op2D['par']['flux'][1]\n",
    "        bg_pars[its-1,:] = op2D['par']['Abg']\n",
    "  \n",
    "        # make new map as old map plus scaled delta map\n",
    "        map_new = map_old+op2D['par']['flux'][1]*delta_map_tot_old/afl_scl\n",
    "    \n",
    "        # same with expectation (data space)\n",
    "        expectation_new = expectation_old + op2D['par']['flux'][1]*conv_delta_map_tot/afl_scl\n",
    "        \n",
    "\n",
    "    except:\n",
    "        # if the fit failed...\n",
    "        # this shouldn't happen too often (or at all)\n",
    "        print('############## Fit failed! proceeding without acceleration ##############')\n",
    "        map_new = map_old + delta_map_tot_old\n",
    "        expectation_new = expectation_old + conv_delta_map_tot\n",
    "    \n",
    "    # check finite values again\n",
    "    if its == 1:\n",
    "        bad_index_init = np.where(np.isnan(map_new) == True)\n",
    "    \n",
    "    # also here\n",
    "    map_new[bad_expo[0],bad_expo[1]] = 0\n",
    "    map_new[np.where(np.isnan(map_new) == True)] = 0\n",
    "    map_iterations[:,:,its] = map_new\n",
    "\n",
    "    # swap maps\n",
    "    map_old = map_new\n",
    "    \n",
    "    # and expectations\n",
    "    expectation_old = expectation_new\n",
    "    \n",
    "    # and repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the fitted background parameter and the map flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(its), [i[0] for i in bg_pars[:its]], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('BG params]')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "map_fluxes = np.zeros(its)\n",
    "for i in range(its):\n",
    "    map_fluxes[i] = np.sum(map_iterations[:,:,i]*domega)\n",
    "    \n",
    "plt.plot(map_fluxes[:its],'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Flux')# [ph/keV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the algorithm converge? Look at the likelihoods.\n",
    "intermediate_lp: Fit likelihoods, i.e. fit quality\n",
    "\n",
    "map_likelihoods: likelihood of maps (vs. initial i.e. basically only background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(its+1)[:-1], intermediate_lp[:its+1][:-1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (intermediate_lp)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(its+1)[:-1], map_likelihoods[:its+1][:-1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (map_likelihoods)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the image!\n",
    "You can loop over all iterations to make a GIF or just show one iteration (usually the final iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import Video\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "from scipy.ndimage import gaussian_filter as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image to plot\n",
    "idx = its \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a color map like viridis (matplotlib default), nipy_spectral, twilight_shifted, etc. Not jet.\n",
    "cmap = plt.get_cmap('viridis') \n",
    "\n",
    "# Bad exposures will be gray\n",
    "cmap.set_bad('lightgray')\n",
    "\n",
    "\n",
    "##################\n",
    "# Select here which pixels should be gray\n",
    "map_iterations_nan = np.copy(map_iterations)\n",
    "\n",
    "# Select also non-zero exposures here to be gray (avoiding the edge effects)\n",
    "# You can play with this. Most success in testing with 1e4, 1e3\n",
    "bad_expo = np.where(expo_map/domega <= 1e4) \n",
    "\n",
    "for i in range(iterations):\n",
    "    map_iterations_nan[bad_expo[0], bad_expo[1], i] = np.nan\n",
    "#################    \n",
    "\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10.24,7.68), subplot_kw={'projection':'aitoff'}, nrows=1, ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120,-60,0,60,120])*deg2rad)\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "plt.xlabel('Gal. Lon. [deg]')\n",
    "plt.ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "\n",
    "\n",
    "# \"ims\" is a list of lists, each row is a list of artists to draw in the\n",
    "# current frame; here we are just animating one artist, the image, in\n",
    "# each frame\n",
    "ims = []\n",
    "\n",
    "\n",
    "# If you want to make a GIF of all iterations:\n",
    "#for i in range(iterations):\n",
    "\n",
    "# If you only want to plot one image:\n",
    "for i in [idx]:\n",
    "\n",
    "    ttl = plt.text(0.5, 1.01, f'RL iteration {i}', horizontalalignment='center', \n",
    "                   verticalalignment='bottom', transform=ax.transAxes)\n",
    "    \n",
    "    # Either gray-out bad exposure (map_iterations_nan) or don't mask (map_iterations)\n",
    "    # Masking out bad exposure \n",
    "    #image = map_iterations_nan[:, :, i]\n",
    "    image = map_iterations[:, :, i]\n",
    "\n",
    "    \n",
    "    img = ax.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,\n",
    "                        \n",
    "                        # Can shift the image along longitude. Here, no shift.\n",
    "                        np.roll(image, axis=1, shift=0),\n",
    "            \n",
    "                        # Optionally smooth with gaussian filter\n",
    "                        #smooth(np.roll(image, axis=1, shift=0), 0.75/pixel_size),\n",
    "                        \n",
    "                        cmap=plt.cm.viridis,\n",
    "                        \n",
    "                        # Optionally set the color scale. Default: linear\n",
    "                        #norm=colors.PowerNorm(0.33)\n",
    "                       )\n",
    "    ax.grid()\n",
    "    \n",
    "    ims.append([img, ttl])\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "#cbar.ax.set_xlabel(r'Flux [10$^{-2}$ ph cm$^{-2}$ s$^{-1}$]')\n",
    "    \n",
    "\n",
    "# Can save a sole image as a PDF \n",
    "#plt.savefig(data_dir + f'images/511keV_RL_image_iteration{idx}.pdf', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "# # Can save all iterations as a GIF\n",
    "# ani = animation.ArtistAnimation(fig, ims, interval=200, blit=True, repeat_delay=0)\n",
    "# ani.save(f'/home/jacqueline/511keV_RL_image_{idx}iterations.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we see?\n",
    "\n",
    "We clearly see the \"bulge\" emission of positron-electron annihilation at the center of the Milky Way. This was also seen in the published image of real COSI-balloon flight data [(Siegert et al. 2020)](https://iopscience.iop.org/article/10.3847/1538-4357/ab9607/meta):\n",
    "\n",
    "<img width=\"600\" alt=\"Siegert_2020_COSI_511keV\" src=\"https://user-images.githubusercontent.com/33991471/196853486-68a90111-245b-442d-841c-756f47c9c14f.png\">\n",
    "\n",
    "\n",
    "The extended disk emission seen in the SPI image above is not visible here. This is expected; SPI saw about 1 photon per week from the disk and has over a decade of observation time. There is not enough data in the 46-day balloon flight to image the disk.\n",
    "\n",
    "However, we can still probe the emission morphology of the bulge by fitting a 2-D Gaussian, for example, to our simulated image. Constraining the parameters of this fit is important for modeling the physics (positron propogation, point sources of positrons, etc.) behind this enduring mystery.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a 2D Gaussian to the emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_2d(xtuple, A, x0, y0, sigma_x, sigma_y, theta):\n",
    "    # theta: rotate the blob by positive, counterclockwise angle theta\n",
    "    (x, y) = xtuple\n",
    "    x0 = float(x0)\n",
    "    y0 = float(y0)\n",
    "    a = np.cos(theta)**2/(2*sigma_x**2) + np.sin(theta)**2/(2*sigma_y**2)\n",
    "    b = np.sin(2*theta)/(4*sigma_x**2) - np.sin(2*theta)/(4*sigma_y**2)\n",
    "    c = np.sin(theta)**2/(2*sigma_x**2) + np.cos(theta)**2/(2*sigma_y**2)\n",
    "    tot = A*np.exp( -( a*(x-x0)**2 + 2*b*(x-x0)*(y-y0) + c*(y-y0)**2 ) )\n",
    "    return tot.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "initial_guess = (2, 0, 0, 10, 10, 4)\n",
    "x = (L_ARRg*deg2rad)[:-1, :-1]\n",
    "y = (B_ARRg*deg2rad)[:-1, :-1]\n",
    "z = map_iterations_nan[:,:,idx]\n",
    "nan = np.isnan(z)\n",
    "z[nan] = 0.\n",
    "popt, pcov = opt.curve_fit(gauss_2d, (x, y), z.ravel(), p0=initial_guess)\n",
    "\n",
    "im_fitted = gauss_2d((x, y), *popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10.24,7.68),subplot_kw={'projection':'aitoff'},nrows=1,ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120,-60,0,60,120])*deg2rad)\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "ax.set_xlabel('Gal. Lon. [deg]')\n",
    "ax.set_ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "# Plot original image\n",
    "ax.pcolormesh(L_ARRg*deg2rad, B_ARRg*deg2rad, z.reshape(len(x), len(x[0])), cmap=plt.cm.viridis)\n",
    "\n",
    "# Plot contours\n",
    "num_contours = 2\n",
    "levels = [np.max(im_fitted)*0.05, np.max(im_fitted)*0.1,\n",
    "          np.max(im_fitted)*0.5, np.max(im_fitted)*0.8]\n",
    "\n",
    "#plt.contour(x, y, im_fitted.reshape(len(x), len(x[0])), levels=num_contours, colors='w')\n",
    "\n",
    "plt.contour(L_ARR*deg2rad, B_ARR*deg2rad, im_fitted.reshape(len(x), len(x[0])), levels = levels, colors='white')\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "#cbar.ax.set_xlabel(r'Flux [10$^{-2}$ ph cm$^{-2}$ s$^{-1}$]')\n",
    "    \n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A:', popt[0])\n",
    "print('x0 [deg]:', popt[1]*180/np.pi)\n",
    "print('y0 [deg]:', popt[2]*180/np.pi)\n",
    "print('sigma_x [deg]:', popt[3]*180/np.pi, '--> FWHM_x [deg]:', 2*np.sqrt(2*np.log(2))*popt[3]*180/np.pi)\n",
    "print('sigma_y [deg]:', popt[4]*180/np.pi, '--> FWHM_y [deg]:', 2*np.sqrt(2*np.log(2))*popt[4]*180/np.pi)\n",
    "print('theta [deg]:', popt[5]*180/np.pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COSIMain",
   "language": "python",
   "name": "cosimain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
